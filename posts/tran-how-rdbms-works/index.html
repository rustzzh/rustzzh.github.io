<!doctype html><html lang=zh-cn><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=robots content="noodp"><meta http-equiv=x-ua-compatible content="IE=edge, chrome=1"><title>[tran]关系型数据库原理 - zh's blog</title><meta name=Description content="(WIP)通过翻译这篇文章达到精读一遍的效果"><meta property="og:title" content="[tran]关系型数据库原理"><meta property="og:description" content="(WIP)通过翻译这篇文章达到精读一遍的效果"><meta property="og:type" content="article"><meta property="og:url" content="https://rustzzh.github.io/posts/tran-how-rdbms-works/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2022-05-09T17:35:56+08:00"><meta property="article:modified_time" content="2022-05-22T14:13:42+00:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="[tran]关系型数据库原理"><meta name=twitter:description content="(WIP)通过翻译这篇文章达到精读一遍的效果"><meta name=application-name content="zh's blog"><meta name=apple-mobile-web-app-title content="zh's blog"><link rel="shortcut icon" type=image/x-icon href=/favicon.ico><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=manifest href=/site.webmanifest><link rel=canonical href=https://rustzzh.github.io/posts/tran-how-rdbms-works/><link rel=prev href=https://rustzzh.github.io/posts/cmu15445-db-storage/><link rel=stylesheet href=/lib/normalize/normalize.min.css><link rel=stylesheet href=/css/style.min.css><link rel=stylesheet href=/lib/fontawesome-free/all.min.css><link rel=stylesheet href=/lib/animate/animate.min.css><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","headline":"[tran]关系型数据库原理","inLanguage":"zh-CN","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/rustzzh.github.io\/posts\/tran-how-rdbms-works\/"},"genre":"posts","keywords":"db, rdbms","wordcount":42002,"url":"https:\/\/rustzzh.github.io\/posts\/tran-how-rdbms-works\/","datePublished":"2022-05-09T17:35:56+08:00","dateModified":"2022-05-22T14:13:42+00:00","publisher":{"@type":"Organization","name":"zh"},"author":{"@type":"Person","name":"zh"},"description":"(WIP)通过翻译这篇文章达到精读一遍的效果"}</script></head><body header-desktop header-mobile><script type=text/javascript>(window.localStorage&&localStorage.getItem("theme")?localStorage.getItem("theme")==="dark":''==="auto"?window.matchMedia("(prefers-color-scheme: dark)").matches:''==="dark")&&document.body.setAttribute("theme","dark")</script><div id=mask></div><div class=wrapper><header class=desktop id=header-desktop><div class=header-wrapper><div class=header-title><a href=/ title="zh's blog">zh's blog</a></div><div class=menu><div class=menu-inner><a class=menu-item href=/posts/>文章 </a><a class=menu-item href=/tags/>标签 </a><a class=menu-item href=/categories/>分类 </a><a class=menu-item href=/friend/>友链 </a><span class="menu-item delimiter"></span><a href=javascript:void(0); class="menu-item theme-switch" title=切换主题>
<i class="fas fa-adjust fa-fw"></i></a></div></div></div></header><header class=mobile id=header-mobile><div class=header-container><div class=header-wrapper><div class=header-title><a href=/ title="zh's blog">zh's blog</a></div><div class=menu-toggle id=menu-toggle-mobile><span></span><span></span><span></span></div></div><div class=menu id=menu-mobile><a class=menu-item href=/posts/ title>文章</a><a class=menu-item href=/tags/ title>标签</a><a class=menu-item href=/categories/ title>分类</a><a class=menu-item href=/friend/ title>友链</a><a href=javascript:void(0); class="menu-item theme-switch" title=切换主题>
<i class="fas fa-adjust fa-fw"></i></a></div></div></header><div class="search-dropdown desktop"><div id=search-dropdown-desktop></div></div><div class="search-dropdown mobile"><div id=search-dropdown-mobile></div></div><main class=main><div class=container><div class=toc id=toc-auto><h2 class=toc-title>目录</h2><div class="toc-content always-active" id=toc-content-auto></div></div><article class="page single"><h1 class="single-title animated flipInX">[tran]关系型数据库原理</h1><div class=post-meta><div class=post-meta-line><span class=post-author><a href=/ title=Author rel=author class=author><i class="fas fa-user-circle fa-fw"></i>zh</a></span></div><div class=post-meta-line><i class="far fa-calendar-alt fa-fw"></i>&nbsp;<time datetime=2022-05-09>2022-05-09</time>&nbsp;<i class="fas fa-pencil-alt fa-fw"></i>&nbsp;约 42002 字&nbsp;
<i class="far fa-clock fa-fw"></i>&nbsp;预计阅读 84 分钟&nbsp;</div></div><div class="details toc" id=toc-static kept><div class="details-summary toc-title"><span>目录</span>
<span><i class="details-icon fas fa-angle-right"></i></span></div><div class="details-content toc-content" id=toc-content-static><nav id=TableOfContents><ul><li><a href=#1-回顾基础>1 回顾基础</a><ul><li><a href=#11-o1-vs-on2>1.1 $O(1)$ vs $O(n^2)$</a><ul><li><a href=#111-概念>1.1.1 概念</a></li><li><a href=#112-例子>1.1.2 例子</a></li><li><a href=#113-更深入一点>1.1.3 更深入一点</a></li></ul></li><li><a href=#12-归并排序>1.2 归并排序</a><ul><li><a href=#121-合并>1.2.1 合并</a></li><li><a href=#122-分治过程>1.2.2 分治过程</a></li><li><a href=#123-排序过程>1.2.3 排序过程</a></li><li><a href=#124-归并排序的强大之处>1.2.4 归并排序的强大之处</a></li></ul></li><li><a href=#13-数组树和哈希表>1.3 数组，树和哈希表</a><ul><li><a href=#131-数组>1.3.1 数组</a></li><li><a href=#132-树和数据库索引>1.3.2 树和数据库索引</a></li><li><a href=#133-哈希表>1.3.3 哈希表</a></li></ul></li></ul></li><li><a href=#2-全局概述>2 全局概述</a></li><li><a href=#3-客户端管理>3 客户端管理</a></li><li><a href=#4-查询管理>4 查询管理</a><ul><li><a href=#41-查询解析>4.1 查询解析</a></li><li><a href=#42-查询重写>4.2 查询重写</a></li><li><a href=#43-统计信息>4.3 统计信息</a></li><li><a href=#44-查询优化器>4.4 查询优化器</a><ul><li><a href=#441-索引>4.4.1 索引</a></li><li><a href=#442-访问路径>4.4.2 访问路径</a></li><li><a href=#443-联结操作>4.4.3 联结操作</a></li><li><a href=#444-简化的案例>4.4.4 简化的案例</a></li><li><a href=#445-动态规划贪心算法和启发式>4.4.5 动态规划、贪心算法和启发式</a></li><li><a href=#446-真正的优化器unimportant-part>4.4.6 真正的优化器(unimportant part)</a></li><li><a href=#447-查询计划缓存>4.4.7 查询计划缓存</a></li></ul></li><li><a href=#45-查询执行>4.5 查询执行</a></li></ul></li><li><a href=#5-数据管理器>5 数据管理器</a><ul><li><a href=#51-缓存管理>5.1 缓存管理</a><ul><li><a href=#511-预取>5.1.1 预取</a></li><li><a href=#512-缓存替换策略>5.1.2 缓存替换策略</a></li><li><a href=#513-写缓存>5.1.3 写缓存</a></li></ul></li><li><a href=#52-事务管理>5.2 事务管理</a><ul><li><a href=#521-酸了>5.2.1 酸了</a></li><li><a href=#522-并发控制>5.2.2 并发控制</a></li><li><a href=#523-锁管理>5.2.3 锁管理</a></li><li><a href=#524-日志管理>5.2.4 日志管理</a></li></ul></li></ul></li><li><a href=#6-总结>6 总结</a></li></ul></nav></div></div><div class=content id=content><div class="details admonition note open"><div class="details-summary admonition-title"><i class="icon fas fa-pencil-alt fa-fw"></i>注意<i class="details-icon fas fa-angle-right fa-fw"></i></div><div class=details-content><div class=admonition-content><p><a href=http://coding-geek.com/how-databases-work/ target=_blank rel="noopener noreffer">原文在这</a></p><p>由于博主英语水平有限，本篇译文以段落为单元，先英后中展示。</p><p>较之原文省略了一些博主认为可以缺少的图片。</p><p><strong>加粗</strong> 用来表示原文中强调的内容， <em>斜体</em> 用于表示博主的想法。</p></div></div></div><blockquote><p>When it comes to relational databases, I can’t help thinking that something is missing. They’re used everywhere. There are many different databases: from the small and useful SQLite to the powerful Teradata. But, there are only a few articles that explain how a database works. You can google by yourself “how does a relational database work” to see how few results there are. Moreover, those articles are short. Now, if you look for the last trendy technologies (Big Data, NoSQL or JavaScript), you’ll find more in-depth articles explaining how they work.</p></blockquote><p>当谈到关系型数据库的时候，我不禁会想少了点啥。
它们使用广泛。
这里有许多种不同的数据库：从小而实用的SQLite到强大的Teradata。
但是很少有文章介绍一个数据库是怎么工作的。
你可以自己谷歌搜索一下"how does a relational database work"看看有多少有信息的文章。
此外这些文章都很短。
与此同时，你去搜索现在时兴的一些技术(大数据，NoSQL或者JS)，你能找到很多对原理解析很有深度的文章。</p><blockquote><p>Are relational databases too old and too boring to be explained outside of university courses, research papers and books?</p></blockquote><p>是否是因为关系型数据库太过老旧无聊，以至于无法在大学课程、论文以及书籍之外来介绍其原理吗？</p><blockquote><p>As a developer, I HATE using something I don’t understand. And, if databases have been used for 40 years, there must be a reason. Over the years, I’ve spent hundreds of hours to really understand these weird black boxes I use every day. Relational Databases are very interesting because they’re based on useful and reusable concepts. If understanding a database interests you but you’ve never had the time or the will to dig into this wide subject, you should like this article.</p></blockquote><p>作为一个开发者，我痛恨实用那些我不理解的技术。
并且数据库被使用了长达四十多年肯定是有原因的。
这些年来我花了数百小时来真正理解这些我每天都在使用的奇怪黑盒。
关系型数据库是非常有趣的，因为其构建基于一些非常有用并且可以复用的概念之上。
如果你对理解一个数据库有点兴趣但你又没有时间或者没有这个意愿来钻研这个广泛的主题，你会喜欢这篇文章的。</p><blockquote><p>Though the title of this article is explicit, the aim of this article is NOT to understand how to use a database. Therefore, you should already know how to write a simple join query and basic CRUD queries; otherwise you might not understand this article. This is the only thing you need to know, I’ll explain everything else.</p></blockquote><p>尽管标题写的很清楚，但还是要强调的是这篇文章的目的 <strong>不是</strong> 让你理解怎么来使用数据库。
因此，你需要有一些基础知识，例如会写简单的join查询、会基本的增删查改，否则你会看得云里雾里。
除此之外的一切内容都会涵盖在这篇文章中。</p><blockquote><p>I’ll start with some computer science stuff like time complexity. I know that some of you hate this concept but, without it, you can’t understand the cleverness inside a database. Since it’s a huge topic, I’ll focus on what I think is essential: the way a database handles an SQL query. I’ll only present the basic concepts behind a database so that at the end of the article you’ll have a good idea of what’s happening under the hood.</p></blockquote><p>我会从类似时间复杂度这样的计算机概念开始介绍。
我知道你们可能讨厌这类概念，但是如果不讲这些概念你们很难理解数据库内部的高明之处。
因为这是一个很大的话题，我会聚焦于介绍 <strong>数据库处理一条SQL的方法</strong> 这条主线。
我仅会介绍数据库内部的一些基本概念，这样你在读完本文后能对数据库内部究竟干了些啥有一个比较清晰的理解。</p><blockquote><p>Since it’s a long and technical article that involves many algorithms and data structures, take your time to read it. Some concepts are more difficult to understand; you can skip them and still get the overall idea.</p></blockquote><p>因为这是一篇涉及了很多算法和数据结构的技术文章，慢慢读吧。
其中的有些概念可能很难理解，你可以暂时地跳过这些部分，不会影响整体的理解。</p><blockquote><p>For the more knowledgeable of you, this article is more or less divided into 3 parts:</p><ul><li>An overview of low-level and high-level database components</li><li>An overview of the query optimization process</li><li>An overview of the transaction and buffer pool management</li></ul></blockquote><p>为了你们更容易理解，这篇文章可以大致分为三个部分：</p><ul><li>对数据库底层和顶层组件的概述</li><li>对查询优化过程的概述</li><li>对事务和缓存池管理的概述</li></ul><h1 id=1-回顾基础>1 回顾基础</h1><blockquote><p>A long time ago (in a galaxy far, far away….), developers had to know exactly the number of operations they were coding. They knew by heart their algorithms and data structures because they couldn’t afford to waste the CPU and memory of their slow computers.</p></blockquote><p>很久以前(在遥远的银河系)，开发人员需要精确的知道自己写的代码需要进行的操作数。
他们对自己的算法和数据结构了然于心，因为他们无法忍受在其低速电脑上浪费CPU和内存。</p><blockquote><p>In this part, I’ll remind you about some of these concepts because they are essential to understand a database. I’ll also introduce the notion of database index.</p></blockquote><p>在这一部分我会带你重温一下这些概念，因为他们对理解数据库来说至关重要。
同时我会向你介绍 <strong>数据库索引</strong> 的概念。</p><h2 id=11-o1-vs-on2>1.1 $O(1)$ vs $O(n^2)$</h2><blockquote><p>Nowadays, many developers don’t care about time complexity … and they’re right!</p></blockquote><p>现在很多开发者不太关心时间复杂度，正确的！</p><blockquote><p>But when you deal with a large amount of data (I’m not talking about thousands) or if you’re fighting for milliseconds, it becomes critical to understand this concept. And guess what, databases have to deal with both situations! I won’t bore you a long time, just the time to get the idea. This will help us later to understand the concept of cost based optimization.</p></blockquote><p>但是当你处理一个量级很大的数据(这里说的不是千级别的)或需要考虑毫秒级别性能问题时，理解这个概念就变得至关重要了。
你猜怎么着，数据库两种情况都需要处理！
我不会在这部分花费你太多时间，仅仅有一个大体印象即可。
这对接下来理解 <strong>基于开销的性能优化</strong> 很关键。</p><h3 id=111-概念>1.1.1 概念</h3><blockquote><p>The time complexity is used to see how long an algorithm will take for a given amount of data. To describe this complexity, computer scientists use the mathematical big O notation. This notation is used with a function that describes how many operations an algorithm needs for a given amount of input data.</p></blockquote><p><strong>时间复杂度用来描述一个算法对于给定数量级的数据需要花费多久的时间来处理</strong>。
计算机科学家们用大O表示法来描述这种复杂度。
这种表示法通常与函数搭配使用，函数用来描述对于一个给定量级的输入数据这个算法需要进行多少次操作。</p><blockquote><p>For example, when I say “this algorithm is in O( some_function() )”, it means that for a certain amount of data the algorithm needs some_function(a_certain_amount_of_data) operations to do its job.</p></blockquote><p>举例来说，当我说这个算法是 <code>O(some_function())</code> 的时候，表示着对于一个给定的量级的数据，这个算法需要 <code>some_function(a_certain_amount_of_data)</code> 次操作才能执行完毕。</p><blockquote><p>What’s important is not the amount of data but the way the number of operations increases when the amount of data increases. The time complexity doesn’t give the exact number of operations but a good idea.</p></blockquote><p>这里关键点并不是数据的量级，而是 <strong>操作数随数据增长时的增长量</strong> 。
时间复杂度并没有给出具体的操作次数而是描述了一个大致量。</p><figure><a class=lightgallery href=/images/TimeComplexity.png title=/images/TimeComplexity.png data-thumbnail=/images/TimeComplexity.png data-sub-html="<h2>时间复杂度</h2>"><img class=lazyload src=/svg/loading.min.svg data-src=/images/TimeComplexity.png data-srcset="/images/TimeComplexity.png, /images/TimeComplexity.png 1.5x, /images/TimeComplexity.png 2x" data-sizes=auto alt=/images/TimeComplexity.png height=1500 width=1500></a><figcaption class=image-caption>时间复杂度</figcaption></figure><blockquote><p>In this figure, you can see the evolution of different types of complexities. I used a logarithmic scale to plot it. In other words, the number of data is quickly increasing from 1 to 1 billion. We can see that:</p><ul><li>The O(1) or constant complexity stays constant (otherwise it wouldn’t be called constant complexity).</li><li>The O(log(n)) stays low even with billions of data.</li><li>The worst complexity is the O(n2) where the number of operations quickly explodes.</li><li>The two other complexities are quickly increasing.</li></ul></blockquote><p>在上面的图片中可以看到不同类型复杂度的演化。使用的是对数刻度。
换言之，数据的量级从1到10亿快速增长。
我们可以看到：</p><ul><li><div id=id-1><strong>$O(1)$</strong> 或者说常量复杂度随着数据量增长保持常量(不然它也不会叫常量复杂度)</div></li><li><div id=id-2><strong>$O(log(n))$</strong> 即使在十亿的数据量下操作增长也很慢</div></li><li><div id=id-3>最差的复杂度是 <strong>$O(n^2)$</strong> ，操作的数量爆炸增长</div></li><li>其他两种复杂度增长迅速</li></ul><h3 id=112-例子>1.1.2 例子</h3><blockquote><p>With a low amount of data, the difference between O(1) and O(n2) is negligible. For example, let’s say you have an algorithm that needs to process 2000 elements.</p><ul><li>An $O(1)$ algorithm will cost you 1 operation</li><li>An $O(log(n))$ algorithm will cost you 7 operations</li><li>An $O(n)$ algorithm will cost you 2 000 operations</li><li>An $O(n*log(n))$ algorithm will cost you 14 000 operations</li><li>An $O(n^2)$ algorithm will cost you 4 000 000 operations</li></ul></blockquote><p>在数据很少的时候，$O(1)$和$O(n^2)$的差距可以忽略。举个例子，当一个算法需要处理2000个元素的时候。</p><ul><li>$O(1)$的算法需要进行 1 次操作</li><li>$O(log(n))$的算法需要进行 7 次操作</li><li>$O(n)$的算法需要进行 2 000 次操作</li><li>$O(n*log(n))$的算法需要进行 14 000 次操作</li><li>$O(n^2)$的算法需要进行 4 000 000 次操作</li></ul><blockquote><p>The difference between $O(1)$ and $O(n^2)$ seems a lot (4 million) but you’ll lose at max 2 ms, just the time to blink your eyes. Indeed, current processors can handle hundreds of millions of operations per second. This is why performance and optimization are not an issue in many IT projects.</p></blockquote><p>这里的$O(1)$和$O(n^2)$看起来差距很大(4 000 000 : 1)但是性能损耗最大2ms，一眨眼的功夫。
现在的处理器(本文写于2015年)每秒可以进行数十亿次操作。
这就是为什么在很多IT项目中性能和优化并不是主要问题。</p><blockquote><p>As I said, it’s still important to know this concept when facing a huge number of data. If this time the algorithm needs to process 1 000 000 elements (which is not that big for a database):</p><ul><li>An $O(1)$ algorithm will cost you 1 operation</li><li>An $O(log(n))$ algorithm will cost you 14 operations</li><li>An $O(n)$ algorithm will cost you 1 000 000 operations</li><li>An $O(n*log(n))$ algorithm will cost you 14 000 000 operations</li><li>An $O(n^2)$ algorithm will cost you 1 000 000 000 000 operations</li></ul></blockquote><p>如我所说，当面对一个量级相当大的数据时，时间复杂度的概念就相当重要了。
如果这次这个算法需要处理一百万个元素(对数据库来说这个数量级都还不算大)：</p><ul><li>$O(1)$的算法需要进行 1 次操作</li><li>$O(log(n))$的算法需要进行 14 次操作</li><li>$O(n)$的算法需要进行 1 000 000 次操作</li><li>$O(n*log(n))$的算法需要进行 14 000 000次操作</li><li>$O(n^2)$的算法需要进行 1 000 000 000 000 次操作</li></ul><blockquote><p>I didn’t do the math but I’d say with the $O(n2)$ algorithm you have the time to take a coffee (even a second one!). If you put another 0 on the amount of data, you’ll have the time to take a long nap.</p></blockquote><p>我没有具体算过，但是这一次$O(n^2)$的算法都够你喝杯咖啡了(甚至喝两杯！)如果数量级再加个0，甚至能睡个午觉。</p><h3 id=113-更深入一点>1.1.3 更深入一点</h3><blockquote><p>To give you an idea:</p><ul><li>A search in a good hash table gives an element in O(1)</li><li>A search in a well-balanced tree gives a result in O(log(n))</li><li>A search in an array gives a result in O(n)</li><li>The best sorting algorithms have an O(n*log(n)) complexity.</li><li>A bad sorting algorithm has an O(n2) complexity</li></ul></blockquote><p>为了让你对时间复杂度的有一个大致的印象：</p><ul><li>在一个比较好的哈希表中执行查找的时间复杂度是$O(1)$</li><li>在一个平衡地很好的树中执行查找的时间复杂度是$O(log(n))$</li><li>在数组中执行查找的时间复杂度是$O(n)$</li><li>最好的排序算法时间复杂度是$O(n*log(s))$</li><li>捞的排序算法时间复杂度是$O(n^2)$</li></ul><blockquote><p>Note: In the next parts, we’ll see these algorithms and data structures.</p></blockquote><p>注：下一小节就会看一下这些算法喝数据结构。</p><blockquote><p>There are multiple types of time complexity:</p><ul><li>the average case scenario</li><li>the best case scenario</li><li>and the worst case scenario</li></ul></blockquote><p>关于时间复杂度有几种类型：</p><ul><li>平均的时间复杂度</li><li>最好的情况对应的复杂度</li><li>最坏的情况对应的复杂度</li></ul><blockquote><p>The time complexity is often the worst case scenario.</p></blockquote><p>一个算法的时间复杂度通常指的是最坏情况的时间复杂度。</p><blockquote><p>I only talked about time complexity but complexity also works for:</p><ul><li>the memory consumption of an algorithm</li><li>the disk I/O consumption of an algorithm</li></ul></blockquote><p>这里我只谈到了时间复杂度，复杂度同样适合于描述：</p><ul><li>一个算法的内存开销</li><li>一个算法的磁盘IO开销</li></ul><blockquote><p>Of course there are worse complexities than $n^2$, like:</p><ul><li>$n^4$: that sucks! Some of the algorithms I’ll mention have this complexity.</li><li>$3^n$: that sucks even more! One of the algorithms we’re going to see in the middle of this article has this complexity (and it’s really used in many databases).</li><li>factorial n : you’ll never get your results, even with a low amount of data.</li><li>$n^n$: if you end-up with this complexity, you should ask yourself if IT is really your field…</li></ul></blockquote><p>当然还有比$n^2$更捞的时间复杂度，如下：</p><ul><li>$n^4$：捞得不谈，有的算法是这个时间复杂度。</li><li>$3^n$：捞得淌口水，文章中部有一个算法是这个复杂度。</li><li>$n!$：时间的尽头。</li><li>$n^n$：如果你写出这样的时间复杂度的算法，只能说一眼顶真，鉴定为不适合写代码。</li></ul><p><em>读到这里的朋友不妨自己写一个$n^n$的算法试试</em></p><blockquote><p>Note: I didn’t give you the real definition of the big O notation but just the idea. You can read this article on Wikipedia for the real (asymptotic) definition.</p></blockquote><p>注：这里我并没有给出关于大O表示法的严谨定义。你可以通过<a href=https://en.wikipedia.org/wiki/Big_O_notation target=_blank rel="noopener noreffer">wikipedia</a>看看更准确的定义。</p><h2 id=12-归并排序>1.2 归并排序</h2><blockquote><p>What do you do when you need to sort a collection? What? You call the sort() function … ok, good answer… But for a database you have to understand how this sort() function works.</p></blockquote><p>当你需要一个有序的元素集的时候需要干些啥呢？啥？你调用<code>sort()</code>&mldr;彳亍，好回答&mldr;但是对于数据库来说需要理解<code>sort()</code>是怎么运行的。</p><blockquote><p>There are several good sorting algorithms so I’ll focus on the most important one: the merge sort. You might not understand right now why sorting data is useful but you should after the part on query optimization. Moreover, understanding the merge sort will help us later to understand a common database join operation called the merge join.</p></blockquote><p>有很多不错的排序算法，这里聚焦于介绍 <strong>归并排序</strong> 。
可能现在你不知道为什么排序算法为什么重要，但看完查询优化那一节之后你应该就理解了。
除此之外，理解归并排序有助于我们之后来理解数据库中的一种常见操作：<strong>合并联结</strong>。</p><h3 id=121-合并>1.2.1 合并</h3><blockquote><p>Like many useful algorithms, the merge sort is based on a trick: merging 2 sorted arrays of size N/2 into a N-element sorted array only costs N operations. This operation is called a merge.</p></blockquote><p>像很多使用的算法一样，归并排序是基于这样一个点：将两个长度为 N/2 的有序数据合并成一个长度为 N 的有序数组只需要 N 次操作。
这个操作叫做 <strong>合并</strong>。</p><blockquote><p>Let’s see what this means with a simple example:</p></blockquote><p>看个例子：</p><figure><a class=lightgallery href=/images/merge_sort_3.png title=/images/merge_sort_3.png data-thumbnail=/images/merge_sort_3.png data-sub-html="<h2>归并</h2>"><img class=lazyload src=/svg/loading.min.svg data-src=/images/merge_sort_3.png data-srcset="/images/merge_sort_3.png, /images/merge_sort_3.png 1.5x, /images/merge_sort_3.png 2x" data-sizes=auto alt=/images/merge_sort_3.png height=500 width=500></a><figcaption class=image-caption>归并</figcaption></figure><blockquote><p>You can see on this figure that to construct the final sorted array of 8 elements, you only need to iterate one time in the 2 4-element arrays. Since both 4-element arrays are already sorted:</p><ol><li>you compare both current elements in the 2 arrays (current=first for the first time)</li><li>then take the lowest one to put it in the 8-element array</li><li>and go to the next element in the array you took the lowest element
and repeat 1,2,3 until you reach the last element of one of the arrays.</li></ol><p>Then you take the rest of the elements of the other array to put them in the 8-element array.</p></blockquote><p>从图片中能看到想要得到一个有序的 8 个元素的数组，需要遍历 2 个有序的 4 元素数组，因为这 2 个数组都是有序的：</p><ol><li>两个数组都从头遍历起，每次遍历比较当前二者的大小</li><li>将更小的那个数据塞入到 8 元素数组中</li><li>将第 2 步取数据的数组的遍历指针往后移一位，重复 1,2,3 步直到遍历完某一个数组</li></ol><p>将剩余的元素依次塞入这个 8 元素的数组。</p><blockquote><p>This works because both 4-element arrays are sorted and therefore you don’t need to “go back” in these arrays.</p></blockquote><p>这样行得通的原因是 2 个 4 元素的数组都是有序的，所以无需在这些数组中“回溯”。</p><blockquote><p>Now that we’ve understood this trick, here is my pseudocode of the merge sort.</p></blockquote><p>现在我们已经理解了这个技巧，下面是归并排序的伪代码</p><pre tabindex=0><code>array mergeSort(array a)
   if(length(a)==1)
      return a[0];
   end if
 
   //recursive calls
   [left_array right_array] := split_into_2_equally_sized_arrays(a);
   array new_left_array := mergeSort(left_array);
   array new_right_array := mergeSort(right_array);
 
   //merging the 2 small ordered arrays into a big one
   array result := merge(new_left_array,new_right_array);
   return result;
</code></pre><blockquote><p>The merge sort breaks the problem into smaller problems then finds the results of the smaller problems to get the result of the initial problem (note: this kind of algorithms is called divide and conquer). If you don’t understand this algorithm, don’t worry; I didn’t understand it the first time I saw it. If it can help you, I see this algorithm as a two-phase algorithm:</p><ul><li>The division phase where the array is divided into smaller arrays</li><li>The sorting phase where the small arrays are put together (using the merge) to form a bigger array.</li></ul></blockquote><p>归并排序把问题拆成很多子问题，然后通过解决子问题然后获得最初的大问题的答案(注：这种类型的算法又称为分治算法)。
如果不懂没关系；我第一次看的时候也没看懂。
或许这种解释可以帮助你理解，将其拆分成两个阶段理解：</p><ol><li>拆分阶段：数组被拆分成更小的数组</li><li>排序阶段：小数组被合并成一个大数组</li></ol><h3 id=122-分治过程>1.2.2 分治过程</h3><figure><a class=lightgallery href=/images/merge_sort_1.png title=/images/merge_sort_1.png data-thumbnail=/images/merge_sort_1.png data-sub-html="<h2>拆分</h2>"><img class=lazyload src=/svg/loading.min.svg data-src=/images/merge_sort_1.png data-srcset="/images/merge_sort_1.png, /images/merge_sort_1.png 1.5x, /images/merge_sort_1.png 2x" data-sizes=auto alt=/images/merge_sort_1.png height=500 width=500></a><figcaption class=image-caption>拆分</figcaption></figure><blockquote><p>During the division phase, the array is divided into unitary arrays using 3 steps. The formal number of steps is log(N) (since N=8, log(N) = 3).</p></blockquote><p>在拆分阶段，数组在 3 步内被拆分成单元素的数组。拆分的次数是$log(N)$。</p><blockquote><p>How do I know that?
<del>I’m a genius!</del> In one word: mathematics. The idea is that each step divides the size of the initial array by 2. The number of steps is the number of times you can divide the initial array by two. This is the exact definition of logarithm (in base 2).</p></blockquote><p>我怎么知道的？</p><p><del>我又不是天才</del>！简而言之：数学定义。因为每一步都是将数组拆分成原始数组的 1/2 。因此总步数就是最初的数组能被 2 除以的次数。正好是以2为底的对数的定义。</p><h3 id=123-排序过程>1.2.3 排序过程</h3><figure><a class=lightgallery href=/images/merge_sort_2.png title=/images/merge_sort_2.png data-thumbnail=/images/merge_sort_2.png data-sub-html="<h2>排序</h2>"><img class=lazyload src=/svg/loading.min.svg data-src=/images/merge_sort_2.png data-srcset="/images/merge_sort_2.png, /images/merge_sort_2.png 1.5x, /images/merge_sort_2.png 2x" data-sizes=auto alt=/images/merge_sort_2.png height=500 width=500></a><figcaption class=image-caption>排序</figcaption></figure><blockquote><p>In the sorting phase, you start with the unitary arrays. During each step, you apply multiple merges and the overall cost is N=8 operations:</p><ul><li>In the first step you have 4 merges that cost 2 operations each</li><li>In the second step you have 2 merges that cost 4 operations each</li><li>In the third step you have 1 merge that costs 8 operations</li></ul><p>Since there are log(N) steps, the overall costs N * log(N) operations</p></blockquote><p>在排序阶段，从单个元素的数组开始。在每一步中你需要运行多次合并，并且总合并次数是 N=8 次：</p><ul><li>第一步需要进行 4 次合并，每次需要执行 2 次操作</li><li>第二步需要进行 2 次合并，每次需要执行 4 次操作</li><li>第三步需要执行 1 次合并，每次需要执行 8 次操作</li></ul><p>总共 log(N) 步，因此总开销是 <strong>N * log(N)</strong> 次操作。</p><h3 id=124-归并排序的强大之处>1.2.4 归并排序的强大之处</h3><blockquote><p>Why this algorithm is so powerful?</p></blockquote><p>为什么这个算法这么猛？</p><blockquote><p>Because:</p><ul><li>You can modify it in order to reduce the memory footprint, in a way that you don’t create new arrays but you directly modify the input array.</li></ul><p>Note: this kind of algorithms is called in-place.</p><ul><li>You can modify it in order to use disk space and a small amount of memory at the same time without a huge disk I/O penalty. The idea is to load in memory only the parts that are currently processed. This is important when you need to sort a multi-gigabyte table with only a memory buffer of 100 megabytes.</li></ul><p>Note: this kind of algorithms is called external sorting.</p><ul><li>You can modify it to run on multiple processes/threads/servers.
For example, the distributed merge sort is one of the key components of Hadoop (which is THE framework in Big Data).</li><li>This algorithm can turn lead into gold (true fact!).</li></ul></blockquote><p>因为：</p><ul><li>你可以通过一点修改来减少其内存开销，实现方式为不再创建新数组而是直接修改原数组。</li></ul><p>注：这种算法被称为<a href=https://en.wikipedia.org/wiki/In-place_algorithm target=_blank rel="noopener noreffer">就地算法</a></p><ul><li>你可以将其改为同时使用磁盘和一小部分内存的方式来避免磁盘IO的巨额开销。这种方式每次仅将当前正在处理的部分读入内存。当需要对一个几GB的大表排序，而内存缓存只有 100 MB时特别有用。</li></ul><p>注：这种算法称作<a href=https://en.wikipedia.org/wiki/External_sorting target=_blank rel="noopener noreffer">外部排序</a>。</p><ul><li>你可以将这个算法改成在多个处理器/线程/服务器上同时运行的。</li></ul><p>例如分布式归并排序就是<a href=https://hadoop.apache.org/docs/stable/api/org/apache/hadoop/mapreduce/Reducer.html target=_blank rel="noopener noreffer">Hadoop</a>(大数据框架)的一个关键组件。</p><ul><li>这种算法可以点石成金(触读的)</li></ul><blockquote><p>This sorting algorithm is used in most (if not all) databases but it’s not the only one. If you want to know more, you can read this research paper that discusses the pros and cons of the common sorting algorithms in a database.</p></blockquote><p>这个算法被用在大多数(但不是全部)数据库上，除了这个算法外还有其他的也被用到了。
如果了解更多关于排序算法在数据库中的应用，你可以读一下<a href=http://wwwlgis.informatik.uni-kl.de/archiv/wwwdvs.informatik.uni-kl.de/courses/DBSREAL/SS2005/Vorlesungsunterlagen/Implementing_Sorting.pdf target=_blank rel="noopener noreffer">这篇研究报告</a>，
在报告中讨论了数据库中常见的几种排序算法的优缺点。</p><h2 id=13-数组树和哈希表>1.3 数组，树和哈希表</h2><blockquote><p>Now that we understand the idea behind time complexity and sorting, I have to tell you about 3 data structures. It’s important because they’re the backbone of modern databases. I’ll also introduce the notion of database index.</p></blockquote><p>至此我们讨论完了时间复杂度和排序，接下来要介绍的是三种数据结构。这些数据结构都是现代数据库的支柱，因此相当重要。
这一节同样会介绍数据库索引的概念。</p><h3 id=131-数组>1.3.1 数组</h3><blockquote><p>The two-dimensional array is the simplest data structure. A table can be seen as an array. For example:</p></blockquote><p>二维数组是最简单的数据结构。一张表就可以被看作一个二维数组，如下图所示：</p><figure><a class=lightgallery href=/images/array.png title=/images/array.png data-thumbnail=/images/array.png data-sub-html="<h2>数组</h2>"><img class=lazyload src=/svg/loading.min.svg data-src=/images/array.png data-srcset="/images/array.png, /images/array.png 1.5x, /images/array.png 2x" data-sizes=auto alt=/images/array.png height=500 width=500></a><figcaption class=image-caption>数组</figcaption></figure><blockquote><p>This 2-dimensional array is a table with rows and columns:</p><ul><li>Each row represents a subject</li><li>The columns the features that describe the subjects.</li><li>Each column stores a certain type of data (integer, string, date …).</li></ul></blockquote><p>二维数组是一个有着行、列的表：</p><ul><li>每一行表示一个主体</li><li>每一列标示一个主体的某个特征</li><li>每一列存储着一个特定的数据类型(整型，字符串，日期&mldr;)</li></ul><blockquote><p>Though it’s great to store and visualize data, when you need to look for a specific value it sucks.</p></blockquote><p>尽管这样存储数据非常直观，但当你想要查找一个特定数据的时候很恶心。</p><blockquote><p>For example, if you want to find all the guys who work in the UK, you’ll have to look at each row to find if the row belongs to the UK. This will cost you N operations (N being the number of rows) which is not bad but could there be a faster way? This is where trees come into play.</p></blockquote><p>例如，<strong>你想找到所有在UK工作的人</strong>，你需要逐行查看这一行是否属于UK。<strong>这个操作的时间复杂度是O(N)</strong>，虽然不是太慢，但是还可以更快。这就是接下来要介绍的树要干的事。</p><h3 id=132-树和数据库索引>1.3.2 树和数据库索引</h3><blockquote><p>A binary search tree is a binary tree with a special property, the key in each node must be:</p><ul><li>greater than all keys stored in the left sub-tree</li><li>smaller than all keys stored in the right sub-tree</li></ul></blockquote><p>二分搜索树是一种特定的二叉树，每个节点满足如下特性：</p><ul><li>比左子树所有的节点的值大</li><li>比右子树所有的节点要小</li></ul><figure><a class=lightgallery href=/images/BST.png title=/images/BST.png data-thumbnail=/images/BST.png data-sub-html="<h2>一颗理想的二分查找树</h2>"><img class=lazyload src=/svg/loading.min.svg data-src=/images/BST.png data-srcset="/images/BST.png, /images/BST.png 1.5x, /images/BST.png 2x" data-sizes=auto alt=/images/BST.png height=500 width=500></a><figcaption class=image-caption>一颗理想的二分查找树</figcaption></figure><blockquote><p>This tree has N=15 elements. Let’s say I’m looking for 208:</p><ul><li>I start with the root whose key is 136. Since 136&lt;208, I look at the right sub-tree of the node 136.</li><li>398>208 so, I look at the left sub-tree of the node 398</li><li>250>208 so, I look at the left sub-tree of the node 250</li><li>200&lt;208 so, I look at the right sub-tree of the node 200. But 200 doesn’t have a right subtree, the value doesn’t exist (because if it did exist it would be in the right subtree of 200)</li></ul></blockquote><p>这棵树有 N=15 个元素。比如说我现在要找值为208的元素：</p><ul><li>从根节点136开始。因为136比208小，所以到右子树去。</li><li>398>208，所以到398的左子树</li><li>250>208，所以到250的左子树</li><li>200&lt;208，所以尝试寻找200的右子树，但是200没有右子树，证明208 <strong>不在这棵树</strong> 中(因为如果存在的话它会出现在200的右子树中)。</li></ul><blockquote><p>Now let’s say I’m looking for 40</p><ul><li>I start with the root whose key is 136. Since 136>40, I look at the left sub-tree of the node 136.</li><li>80>40 so, I look at the left sub-tree of the node 80</li><li>40= 40, the node exists. I extract the id of the row inside the node (it’s not in the figure) and look at the table for the given row id.</li><li>Knowing the row id let me know where the data is precisely on the table and therefore I can get it instantly.</li></ul></blockquote><p>现在来找40:</p><ul><li>136>40，去左子树</li><li>80>40，去左子树</li><li>40==40，<strong>节点存在</strong>。从其中取出行id，并到表中查询相应的行。</li><li>因为知道了行id相当于知道了行在表中的具体位置，所以此时取数据很快。</li></ul><blockquote><p>In the end, both searches cost me the number of levels inside the tree. If you read carefully the part on the merge sort you should see that there are log(N) levels. So the cost of the search is log(N), not bad!</p></blockquote><p>可以看到，这两次查询的开销都是树高。
如果你在归并排序那一章读得比较仔细的你应该会发现树高是 log(N).
所以查询的开销是 log(N)，还不错。</p><blockquote><p>Back to our problem
But this stuff is very abstract so let’s go back to our problem. Instead of a stupid integer, imagine the string that represents the country of someone in the previous table. Suppose you have a tree that contains the column “country” of the table:</p><ul><li>If you want to know who is working in the UK</li><li>you look at the tree to get the node that represents the UK</li><li>inside the “UK node” you’ll find the locations of the rows of the UK workers.</li></ul></blockquote><p><strong>回到之前的问题</strong></p><p>但这个数据结构对如何解决开头提到的问题(找到所有在UK工作的人)十分抽象。只需要将树中存储的值换成一个表示某个人工作城市的字符串即可。
假设你有一颗存储着表中“城市”列值的树：</p><ul><li>如果你想知道谁在UK工作</li><li>在树中查找代表着UK的节点</li><li>在“UK节点”中你会找到在UK工作的人的信息在表中的位置</li></ul><blockquote><p>This search only costs you log(N) operations instead of N operations if you directly use the array. What you’ve just imagined was a database index.</p></blockquote><p>这个搜索仅仅需要log(N)次操作，而你如果直接去表中查需要N次。
这就是<strong>数据库索引</strong>。</p><blockquote><p>You can build a tree index for any group of columns (a string, an integer, 2 strings, an integer and a string, a date …) as long as you have a function to compare the keys (i.e. the group of columns) so that you can establish an order among the keys (which is the case for any basic types in a database).</p></blockquote><p>你可以为任何的列组构建这样的树索引(一个字符串、一个整数、两个字符串、一个整数和一个字符串、时间等等)只要你能提供一个比较函数来对这个列组进行排序，基于这个排序能够在这些键中建立起索引树。</p><p><strong>B+树索引</strong></p><blockquote><p>Although this tree works well to get a specific value, there is a BIG problem when you need to get multiple elements between two values. It will cost O(N) because you’ll have to look at each node in the tree and check if it’s between these 2 values (for example, with an in-order traversal of the tree). Moreover this operation is not disk I/O friendly since you’ll have to read the full tree. We need to find a way to efficiently do a range query. To answer this problem, modern databases use a modified version of the previous tree called B+Tree. In a B+Tree:</p><ul><li>only the lowest nodes (the leaves) store information (the location of the rows in the associated table)</li><li>the other nodes are just here to route to the right node during the search.</li></ul></blockquote><p>尽管这个树在单点查询的时候还不赖，但需要获取 <strong>两个值之间的所有元素</strong> 的时候会遇到大问题。
范围查询时的时间复杂度是 O(N)，因为你需要遍历整棵树来看这个节点是不是在两个值之间(例如对树进行中序遍历)。
除此之外，这个操作也不是磁盘IO友好型的，因为需要把整棵树都读到内存。
我们需要找到一种高效执行 <strong>范围查询</strong> 的方式。
为了解决这个问题，现代数据库使用了一种更高级的二叉搜索树：B+树。
在B+树中：</p><ul><li>只有叶子结点真正 <strong>存储信息</strong></li><li>其他节点(根、中间节点)只是在搜索中起 <strong>路由</strong> 作用。</li></ul><figure><a class=lightgallery href=/images/database_index.png title=/images/database_index.png data-thumbnail=/images/database_index.png data-sub-html="<h2>数据库索引</h2>"><img class=lazyload src=/svg/loading.min.svg data-src=/images/database_index.png data-srcset="/images/database_index.png, /images/database_index.png 1.5x, /images/database_index.png 2x" data-sizes=auto alt=/images/database_index.png height=500 width=500></a><figcaption class=image-caption>数据库索引</figcaption></figure><blockquote><p>As you can see, there are more nodes (twice more). Indeed, you have additional nodes, the “decision nodes” that will help you to find the right node (that stores the location of the rows in the associated table). But the search complexity is still in O(log(N)) (there is just one more level). The big difference is that the lowest nodes are linked to their successors.</p></blockquote><p>如图所示，节点更多了(两倍多)。确实，除数据节点外多了很多“决策节点”，这些决策节点帮你找到正确的信息节点。
但是搜索的复杂度仍然是 log(N)，只多了一层。
最大的区别是各个叶子结点可以看成有序链表。</p><blockquote><p>With this B+Tree, if you’re looking for values between 40 and 100:</p><ul><li>You just have to look for 40 (or the closest value after 40 if 40 doesn’t exist) like you did with the previous tree.</li><li>Then gather the successors of 40 using the direct links to the successors until you reach 100.</li></ul></blockquote><p>有了B+树后，如果你想取出所有40～100的值：</p><ul><li>你只需要像之前一样找到40(如果40不在，则找到第一个比40大的)</li><li>然后直接在有序链表中一直遍历到第一个大于100的节点结束</li></ul><blockquote><p>Let’s say you found M successors and the tree has N nodes. The search for a specific node costs log(N) like the previous tree. But, once you have this node, you get the M successors in M operations with the links to their successors. This search only costs M + log(N) operations vs N operations with the previous tree. Moreover, you don’t need to read the full tree (just M + log(N) nodes), which means less disk usage. If M is low (like 200 rows) and N large (1 000 000 rows) it makes a BIG difference.</p></blockquote><p>不妨假设有M个符合条件的节点并且树一共有N个节点。
在树中找到一个特定节点需要进行 log(N) 次操作。
但是找到这个节点后只需要进行M次操作就能完成这次查询。
总开销是M+log(N)，而在之前的树中需要进行N次操作。
除此之外，你不需要讲整棵树都读到内存中(只需要读M+log(N)个节点)，在磁盘IO上开销更小。
如果M相对较小(e.g.200)，N相对很大(e.g. 1 000 000)，二者的查询效率差距很大。</p><blockquote><p>But there are new problems (again!). If you add or remove a row in a database (and therefore in the associated B+Tree index):</p><ul><li>you have to keep the order between nodes inside the B+Tree otherwise you won’t be able to find nodes inside the mess.</li><li>you have to keep the lowest possible number of levels in the B+Tree otherwise the time complexity in O(log(N)) will become O(N).</li></ul></blockquote><p>但这样又有问题了。你在数据库中进行插入或删除的时候(会影响到相应的B+树索引)：</p><ul><li>你必须保持B+树中节点的有序性</li><li>你必须使B+树的层级尽可能的低，不然时间复杂度又会变成O(N)。</li></ul><blockquote><p>In other words, the B+Tree needs to be self-ordered and self-balanced. Thankfully, this is possible with smart deletion and insertion operations. But this comes with a cost: the insertion and deletion in a B+Tree are in O(log(N)). This is why some of you have heard that using too many indexes is not a good idea. Indeed, you’re slowing down the fast insertion/update/deletion of a row in a table since the database needs to update the indexes of the table with a costly O(log(N)) operation per index. Moreover, adding indexes means more workload for the transaction manager (we will see this manager at the end of the article).</p></blockquote><p>换言之，B+树需要是自排序和自平衡的。
值得庆幸的是，通过一种巧妙的插入、删除操作是可以做到的。
但是这对于写操作来说引入了额外的开销：在B+树中的插入和删除操作是O(log(N))的。
这就是为什么你有时候会听到 <strong>建太多索引不太好</strong> 。
确实，建立索引的代价是 <strong>拖慢对行的写操作</strong>，因为对表的更新需要同步到索引中。
除此之外，加索引会对 <strong>事务管理器</strong> 带来更多的负担。</p><blockquote><p>For more details, you can look at the Wikipedia article about B+Tree. If you want an example of a B+Tree implementation in a database, look at this article and this article from a core developer of MySQL. They both focus on how innoDB (the engine of MySQL) handles indexes.</p></blockquote><p>更多的细节可以查看这篇关于B+树的<a href=https://en.wikipedia.org/wiki/B%2B_tree target=_blank rel="noopener noreffer">Wikipedia</a>。
如果你想要了解B+树在数据库中的具体实现样例，可以看看<a href=https://blog.jcole.us/2013/01/07/the-physical-structure-of-innodb-index-pages/ target=_blank rel="noopener noreffer">这篇</a>和<a href=https://blog.jcole.us/2013/01/10/btree-index-structures-in-innodb/ target=_blank rel="noopener noreffer">这篇</a>文章，都出自MySQL的核心开发人员。
两篇文章聚焦于innoDB(MySQL的存储引擎)如何处理索引。</p><blockquote><p>Note: I was told by a reader that, because of low-level optimizations, the B+Tree needs to be fully balanced.</p></blockquote><p>注：曾经有读者和我说，因为底层优化，B+树需要完全平衡。</p><h3 id=133-哈希表>1.3.3 哈希表</h3><blockquote><p>Our last important data structure is the hash table. It’s very useful when you want to quickly look for values. Moreover, understanding the hash table will help us later to understand a common database join operation called the hash join. This data structure is also used by a database to store some internal stuff (like the lock table or the buffer pool, we’ll see both concepts later)</p></blockquote><p>最后一个比较重要的数据结构是哈希表。
它在想快速查找值的时候十分有用。
除此之外，理解哈希表能帮助我们之后理解 <strong>哈希联结</strong>。
这个数据结构同样也被数据库用于存储一些内部数据(例如 <strong>锁表</strong> 和 <strong>缓冲池</strong>，之后会谈到这些概念)</p><blockquote><p>The hash table is a data structure that quickly finds an element with its key. To build a hash table you need to define:
a key for your elements
a hash function for the keys. The computed hashes of the keys give the locations of the elements (called buckets).
a function to compare the keys. Once you found the right bucket you have to find the element you’re looking for inside the bucket using this comparison.</p></blockquote><p>哈希表是一个能通过键快速查询到值的数据结构。
为了建立一个哈希表你需要定义：</p><ul><li>元素的 <strong>键</strong></li><li>对键进行哈希的 <strong>哈希函数</strong>。哈希函数能够对输入的键计算出其在桶中的位置。</li><li><strong>一个用于比较键的函数</strong>。一旦找到相应的桶之后，需要利用这个函数来在桶中找到想要的元素。</li></ul><blockquote><p>A simple example
Let’s have a visual example:
举个例子：<figure><a class=lightgallery href=/images/hash_table.png title=/images/hash_table.png data-thumbnail=/images/hash_table.png data-sub-html="<h2>哈希表</h2>"><img class=lazyload src=/svg/loading.min.svg data-src=/images/hash_table.png data-srcset="/images/hash_table.png, /images/hash_table.png 1.5x, /images/hash_table.png 2x" data-sizes=auto alt=/images/hash_table.png height=500 width=500></a><figcaption class=image-caption>哈希表</figcaption></figure></p></blockquote><blockquote><p>This hash table has 10 buckets. Since I’m lazy I only drew 5 buckets but I know you’re smart so I let you imagine the 5 others. The Hash function I used is the modulo 10 of the key. In other words I only keep the last digit of the key of an element to find its bucket:</p><ul><li>if the last digit is 0 the element ends up in the bucket 0,</li><li>if the last digit is 1 the element ends up in the bucket 1,</li><li>if the last digit is 2 the element ends up in the bucket 2,</li><li>…</li></ul></blockquote><p>这个哈希表有 10 个桶。
因为我比较懒所以只画了 5 个桶但我知道你们很聪明所以能够想象出其余的 5 个桶。
这里的哈希函数是对键模 10。
换言之通过键的最后一位来找到相应的桶：</p><ul><li>如果最后一位是 0，那就落入 0 号桶</li><li>如果最后一位是 1，就落入 1 号桶</li><li>以此类推</li></ul><blockquote><p>The compare function I used is simply the equality between 2 integers.</p></blockquote><p>使用的比较函数就是对整数进行大小比较的方法。</p><blockquote><p>Let’s say you want to get the element 78:</p><ul><li>The hash table computes the hash code for 78 which is 8.</li><li>It looks in the bucket 8, and the first element it finds is 78.</li><li>It gives you back the element 78</li><li>The search only costs 2 operations (1 for computing the hash value and the other for finding the element inside the bucket).</li></ul></blockquote><p>以查找 78 为例：</p><ul><li>哈希表计算出 78 的哈希值是 8</li><li>查询 8 号桶，第一个元素就是 78</li><li>因此返回 78</li><li>这次查询仅执行了 2 次操作(一次用于计算哈希值，另一次用于在桶中找到元素)</li></ul><blockquote><p>Now, let’s say you want to get the element 59:</p><ul><li>The hash table computes the hash code for 59 which is 9.</li><li>It looks in the bucket 9, and the first element it finds is 99. Since 99!=59, element 99 is not the right element.</li><li>Using the same logic, it looks at the second element (9), the third (79), … , and the last (29).</li><li>The element doesn’t exist.</li><li>The search costs 7 operations.</li></ul></blockquote><p>现在来找 59</p><ul><li>59 的哈希值是 9</li><li>到 9 号桶进行查找，第一个元素是 99，因为 99!=59，所以 99 不是想要的元素</li><li>使用相同的逻辑在桶中继续查找，9 -> 79 -> 29&mldr;</li><li>遍历完整个桶发现也没有符合条件的，证明元素不存在</li><li>总共执行了 7 次</li></ul><blockquote><p>A good hash function</p></blockquote><p><strong>一个好的哈希函数</strong></p><blockquote><p>As you can see, depending on the value you’re looking for, the cost is not the same!</p></blockquote><p>如你所见，对于不同的值的查询开销并不相同。</p><blockquote><p>If I now change the hash function with the modulo 1 000 000 of the key (i.e. taking the last 6 digits), the second search only costs 1 operation because there are no elements in the bucket 000059. The real challenge is to find a good hash function that will create buckets that contain a very small amount of elements.</p></blockquote><p>如果我将哈希函数改为对键模1 000 000，第二次查找就只需要 1 次操作，因为 000059 号桶中没东西。
<strong>真正的困难点在于找到一个好的哈希函数，这个哈希函数需要保证在每个桶中的元素的量相当小。</strong></p><blockquote><p>In my example, finding a good hash function is easy. But this is a simple example, finding a good hash function is more difficult when the key is:</p><ul><li>a string (for example the last name of a person)</li><li>2 strings (for example the last name and the first name of a person)</li><li>2 strings and a date (for example the last name, the first name and the birth date of a person)</li><li>…</li></ul></blockquote><p>在我举的例子中找到一个好的哈希函数很简单。但这只是一个简单的示例，当键是如下类型的时候找到一个好的哈希函数就难得多了：</p><ul><li>一个字符串(例如一个人的姓)</li><li>两个字符串(例如一个人的姓和名)</li><li>两个字符串以及一个日期(例如姓、名、出生日期)</li></ul><blockquote><p>With a good hash function, the search in a hash table is in O(1).</p></blockquote><p><strong>在你有一个好的哈希函数的时候，在哈希表中查询的复杂度是O(1).</strong></p><blockquote><p>Array vs hash table</p></blockquote><p><strong>数组和哈希表对比</strong></p><blockquote><p>Why not using an array?</p></blockquote><p>为什么不用数组呢？</p><blockquote><p>Hum, you’re asking a good question.</p><ul><li>A hash table can be half loaded in memory and the other buckets can stay on disk.</li><li>With an array you have to use a contiguous space in memory. If you’re loading a large table it’s very difficult to have enough contiguous space.</li><li>With a hash table you can choose the key you want (for example the country AND the last name of a person).</li></ul></blockquote><p>这其实是一个好问题。</p><ul><li>一个哈希表在使用的时候并不需要全部读入内存。</li><li>使用数组的时候你需要在内存中有一块连续的空间，如果你需要将一张很大的表读入内存的话，很难找到这么大一块连续空间</li><li>使用哈希表你可以自己选择键的形式</li></ul><blockquote><p>For more information, you can read my article on the Java HashMap which is an efficient hash table implementation; you don’t need to understand Java to understand the concepts inside this article.</p></blockquote><p>如果你想要了解更多信息，可以读一下我这篇关于<a href=http://coding-geek.com/how-does-a-hashmap-work-in-java/ target=_blank rel="noopener noreffer">Java HashMap</a>实现的文章；读这篇文章不需要会Java就能理解其中的概念。</p><h1 id=2-全局概述>2 全局概述</h1><blockquote><p>We’ve just seen the basic components inside a database. We now need to step back to see the big picture.</p></blockquote><p>上文介绍了数据库内部的常用组件。现在需要退后一步来了解一下整体图景。</p><blockquote><p>A database is a collection of information that can easily be accessed and modified. But a simple bunch of files could do the same. In fact, the simplest databases like SQLite are nothing more than a bunch of files. But SQLite is a well-crafted bunch of files because it allows you to:</p><ul><li>use transactions that ensure data are safe and coherent</li><li>quickly process data even when you’re dealing with millions of data</li></ul></blockquote><p>数据库是数据的集合，通过数据库能够高效地对数据进行读取、修改。
但是一堆简单的文件也能够达到这个效果。
事实上，像SQLite这样最简单的数据库确实就是一堆文件。
但SQLite是一堆经过精心设计的文件，能够让你做到：</p><ul><li>使用事务，并确保数据安全和一致性</li><li>当面对百万级别的数据时仍能快速处理</li></ul><blockquote><p>More generally, a database can be seen as the following figure:</p></blockquote><p>通常，数据库可以用下图描绘：</p><figure><a class=lightgallery href=/images/global_overview.png title=/images/global_overview.png data-thumbnail=/images/global_overview.png data-sub-html="<h2>全局视图</h2>"><img class=lazyload src=/svg/loading.min.svg data-src=/images/global_overview.png data-srcset="/images/global_overview.png, /images/global_overview.png 1.5x, /images/global_overview.png 2x" data-sizes=auto alt=/images/global_overview.png height=500 width=500></a><figcaption class=image-caption>全局视图</figcaption></figure><blockquote><p>Before writing this part, I’ve read multiple books/papers and every source had its on way to represent a database. So, don’t focus too much on how I organized this database or how I named the processes because I made some choices to fit the plan of this article. What matters are the different components; the overall idea is that a database is divided into multiple components that interact with each other.</p></blockquote><p>在写这一章之前我读了很多书、文章，它们对于如何描绘一个数据库采用的方式各不相同。
因此不必过度关注我如何组织数据库或我怎么命名这些过程的，为了适配这篇文章的进度我做了一些调整。
真正重要的是不同的组件；总体的思路是 <strong>数据库被分成了很多组件，这些组件之间发生交互</strong>。</p><blockquote><p>The core components:</p><ul><li>The process manager: Many databases have a pool of processes/threads that needs to be managed. Moreover, in order to gain nanoseconds, some modern databases use their own threads instead of the Operating System threads.</li><li>The network manager: Network I/O is a big issue, especially for distributed databases. That’s why some databases have their own manager.</li><li>File system manager: Disk I/O is the first bottleneck of a database. Having a manager that will perfectly handle the Operating System file system or even replace it is important.</li><li>The memory manager: To avoid the disk I/O penalty a large quantity of ram is required. But if you handle a large amount of memory, you need an efficient memory manager. Especially when you have many queries using memory at the same time.</li><li>Security Manager: for managing the authentication and the authorizations of the users</li><li>Client manager: for managing the client connections</li><li>…</li></ul></blockquote><p>核心组件如下:</p><ul><li><strong>进程管理器</strong>：许多数据库都有 <strong>进程/线程池</strong> 需要被管理。除此之外，许多现代数据库为了获得纳秒级别的性能提升，用其自己实现的线程取代操作系统的线程。</li><li><strong>网络管理器</strong>：网络I/O是个大问题，尤其对于分布式系统而言。这也是为什么一些数据库自己实现了网络IO管理器。</li><li><strong>文件系统管理器</strong>：<strong>磁盘I/O是数据库的首要瓶颈</strong>。因此实现一个管理器来完美处理操作系统文件系统甚至替代之很重要。</li><li><strong>内存管理器</strong>：为了避免磁盘I/O延迟，需要一块很大的内存。但是为了分配这块很大的内存，需要一个高效的内存管理器。尤其是当你同时有大量查询需要使用内存的时候。</li><li><strong>安全管理器</strong>：进行权限管理</li><li><strong>客户端管理器</strong>：管理客户端连接</li></ul><blockquote><p>The tools:</p><ul><li>Backup manager: for saving and restoring a database.</li><li>Recovery manager: for restarting the database in a coherent state after a crash</li><li>Monitor manager: for logging the activity of the database and providing tools to monitor a database</li><li>Administration manager: for storing metadata (like the names and the structures of the tables) and providing tools to manage databases, schemas, tablespaces, …</li><li>…</li></ul></blockquote><p>工具:</p><ul><li><strong>备份管理器</strong>：保存和恢复数据库</li><li><strong>恢复管理器</strong>：用于在崩溃后将数据库重启至一个一致的状态</li><li><strong>监控管理器</strong>：打出数据库活动日志并提供相应工具观测数据库状态</li><li><strong>管理员管理器</strong>：存储元数据(如表结构与表名)并提供工具来管理数据库、模式、表空间等等</li></ul><blockquote><p>The query Manager:</p><ul><li>Query parser: to check if a query is valid</li><li>Query rewriter: to pre-optimize a query</li><li>Query optimizer: to optimize a query</li><li>Query executor: to compile and execute a query</li></ul></blockquote><p>查询管理器:</p><ul><li><strong>查询解析器</strong>：来检查查询请求是否合法</li><li><strong>查询重写器</strong>：对查询进行预优化</li><li><strong>查询优化器</strong>：优化查询</li><li><strong>查询执行器</strong>：来编译和执行查询</li></ul><blockquote><p>The data manager:</p><ul><li>Transaction manager: to handle transactions</li><li>Cache manager: to put data in memory before using them and put data in memory before writing them on disk</li><li>Data access manager: to access data on disk</li></ul></blockquote><p>数据管理器:</p><ul><li><strong>事务管理器</strong>：处理事务</li><li><strong>缓存管理器</strong>：在使用/修改数据之前将数据从磁盘移入内存</li><li><strong>数据访问管理器</strong>：访问磁盘上的数据</li></ul><blockquote><p>For the rest of this article, I’ll focus on how a database manages an SQL query through the following processes:</p><ul><li>the client manager</li><li>the query manager</li><li>the data manager (I’ll also include the recovery manager in this part)</li></ul></blockquote><p>在文章的剩余部分将聚焦于数据库在如下几个过程中如何处理SQL查询：</p><ul><li>客户端管理器</li><li>查询管理器</li><li>数据管理器(在这一部分会介绍一下恢复管理器)</li></ul><h1 id=3-客户端管理>3 客户端管理</h1><figure><a class=lightgallery href=/images/client_manager.png title=/images/client_manager.png data-thumbnail=/images/client_manager.png data-sub-html="<h2>客户端管理器</h2>"><img class=lazyload src=/svg/loading.min.svg data-src=/images/client_manager.png data-srcset="/images/client_manager.png, /images/client_manager.png 1.5x, /images/client_manager.png 2x" data-sizes=auto alt=/images/client_manager.png height=1500 width=1500></a><figcaption class=image-caption>客户端管理器</figcaption></figure><blockquote><p>The client manager is the part that handles the communications with the client. The client can be a (web) server or an end-user/end-application. The client manager provides different ways to access the database through a set of well-known APIs: JDBC, ODBC, OLE-DB …</p></blockquote><p>客户端管理器是用来管理和客户端通信的部分。
客户端可以是网页服务器或是终端用户/终端应用程序。
客户端管理器通过一组知名的API提供不同的访问数据库的方式：JDBC，ODBC，OLE-DB</p><blockquote><p>It can also provide proprietary database access APIs.</p></blockquote><p>它还可以提供数据库专有的访问API。</p><blockquote><p>When you connect to a database:</p><ul><li>The manager first checks your authentication (your login and password) and then checks if you have the authorizations to use the database. These access rights are set by your DBA.</li><li>Then, it checks if there is a process (or a thread) available to manage your query.</li><li>It also checks if the database if not under heavy load.</li><li>It can wait a moment to get the required resources. If this wait reaches a timeout, it closes the connection and gives a readable error message.</li><li>Then it sends your query to the query manager and your query is processed</li><li>Since the query processing is not an “all or nothing” thing, as soon as it gets data from the query manager, it stores the partial results in a buffer and start sending them to you.</li><li>In case of problem, it stops the connection, gives you a readable explanation and releases the resources.</li></ul></blockquote><p>当你连到一个数据库的时候：</p><ul><li>管理器首先进行 <strong>身份验证</strong> (登录的账号和密码)随后检查你是否有数据库 <strong>授权</strong>。这些访问权限都是由DBA设置的。</li><li>然后检查是否有空闲的进程/线程可供你使用</li><li>同时会检查数据库此时是否处于高负载状态</li><li>在获得需要的资源前可能需要等待一段时间，如果等待时间超过某个设定的超时时间它会关闭连接并给客户端返回一个可读的错误信息。</li><li>随后它将你的请求发给查询管理器，此时请求真正开始处理</li><li>因为查询过程并不是一个“要么完成要么失败”的事，因此一旦其收到来自查询管理器的数据，它将这个返回值存在一个缓冲中并开始向客户端发送响应</li><li>如果出现问题，它会停止连接，并返回可读的解释同时释放资源。</li></ul><h1 id=4-查询管理>4 查询管理</h1><figure><a class=lightgallery href=/images/query_manager.png title=/images/query_manager.png data-thumbnail=/images/query_manager.png data-sub-html="<h2>查询管理器</h2>"><img class=lazyload src=/svg/loading.min.svg data-src=/images/query_manager.png data-srcset="/images/query_manager.png, /images/query_manager.png 1.5x, /images/query_manager.png 2x" data-sizes=auto alt=/images/query_manager.png height=1500 width=1500></a><figcaption class=image-caption>查询管理器</figcaption></figure><blockquote><p>This part is where the power of a database lies. During this part, an ill-written query is transformed into a fast executable code. The code is then executed and the results are returned to the client manager. It’s a multiple-step operation:</p><ul><li>the query is first parsed to see if it’s valid</li><li>it’s then rewritten to remove useless operations and add some pre-optimizations</li><li>it’s then optimized to improve the performances and transformed into an execution and data access plan.</li><li>then the plan is compiled</li><li>at last, it’s executed</li></ul></blockquote><p>这部分便是数据库的强大之处了。
在这一部分，写得不好的查询被转换成 <strong>速度很快</strong> 的可执行代码。
然后代码将被执行并且将结果返回给客户端管理器。
这是一个多步操作：</p><ul><li>首先要 <strong>解析</strong> 查询，检查其是否合法</li><li>然后 <strong>重写</strong>，移去无用的操作并加上一些预优化</li><li>随后对其进行 <strong>优化</strong>，来提高性能并将其转化成一次执行以及数据访问计划</li><li>随后计划被 <strong>编译</strong></li><li>最后被 <strong>执行</strong></li></ul><blockquote><p>In this part, I won’t talk a lot about the last 2 points because they’re less important.</p></blockquote><p>在这一部分不会对最后两点进行过多的探讨，因为它们没那么重要。</p><blockquote><p>After reading this part, if you want a better understanding I recommend reading:</p><ul><li>The initial research paper (1979) on cost based optimization: Access Path Selection in a Relational Database Management System. This article is only 12 pages and understandable with an average level in computer science.</li><li>A very good and in-depth presentation on how DB2 9.X optimizes queries here</li><li>A very good presentation on how PostgreSQL optimizes queries here. It’s the most accessible document since it’s more a presentation on “let’s see what query plans PostgreSQL gives in these situations“ than a “let’s see the algorithms used by PostgreSQL”.</li><li>The official SQLite documentation about optimization. It’s “easy” to read because SQLite uses simple rules. Moreover, it’s the only official documentation that really explains how it works.</li><li>A good presentation on how SQL Server 2005 optimizes queries here</li><li>A white paper about optimization in Oracle 12c here</li><li>2 theoretical courses on query optimization from the authors of the book “DATABASE SYSTEM CONCEPTS” here and there. A good read that focuses on disk I/O cost but a good level in CS is required.</li><li>Another theoretical course that I find more accessible but that only focuses on join operators and disk I/O.</li></ul></blockquote><p>读完这一章后，如果你想对其中的内容有更深入的了解，我推荐阅读以下内容：</p><ul><li>最早关于基于开销的优化的研究论文：<a href=https://people.eecs.berkeley.edu/~brewer/cs262/3-selinger79.pdf target=_blank rel="noopener noreffer">Access Path Selection in a Relational Database Management System.</a>.这篇文章只有12页并且只需要一般的计算机知识就能理解。</li><li>一篇关于DB2 9.X如何优化查询的<a href=http://infolab.stanford.edu/~hyunjung/cs346/db2-talk.pdf target=_blank rel="noopener noreffer">演讲</a></li><li>一篇关于PostgreSQL如何优化查询的<a href=http://momjian.us/main/writings/pgsql/optimizer.pdf target=_blank rel="noopener noreffer">演讲</a>。这是最容易理解的文档，因为它更多的在介绍“当面对这些情况的时候PostgreSQL会给出什么查询计划”，而不是介绍“PostgreSQL里面用了什么算法”。</li><li><a href=https://www.sqlite.org/optoverview.html target=_blank rel="noopener noreffer">SQLite关于优化的官方文档</a>。很易读，因为SQLite很简单。除此之外，他是唯一一篇真正解释SQLite如何运行的官方文档。</li><li>一篇关于SQL Server的<a href=https://blogs.msdn.com/cfs-filesystemfile.ashx/__key/communityserver-components-postattachments/00-08-50-84-93/QPTalk.pdf target=_blank rel="noopener noreffer">演讲</a></li><li>一篇关于Oracle 12c优化的<a href=http://www.oracle.com/technetwork/database/bi-datawarehousing/twp-optimizer-with-oracledb-12c-1963236.pdf target=_blank rel="noopener noreffer">白皮书</a></li><li>两门关于查询优化的理论课，出自帆船书的作者，<a href=http://codex.cs.yale.edu/avi/db-book/db6/slide-dir/PPT-dir/ch12.ppt target=_blank rel="noopener noreffer">A</a><a href=http://codex.cs.yale.edu/avi/db-book/db6/slide-dir/PPT-dir/ch13.ppt target=_blank rel="noopener noreffer">B</a>。很好的介绍了磁盘IO开销，但需要读者有较好的计算机基础。</li><li>另一个更容易理解的<a href=https://www.informatik.hu-berlin.de/de/forschung/gebiete/wbi/teaching/archive/sose05/dbs2/slides/09_joins.pdf target=_blank rel="noopener noreffer">理论课</a>，聚焦于联结操作和磁盘IO。</li></ul><h2 id=41-查询解析>4.1 查询解析</h2><blockquote><p>Each SQL statement is sent to the parser where it is checked for correct syntax. If you made a mistake in your query the parser will reject the query. For example, if you wrote “SLECT …” instead of “SELECT …”, the story ends here.</p></blockquote><p>每个SQL语句都会被送到解析器，在这里检查其语法正确性。
如果查询中存在错误解析器会拒绝请求。
例如把“SELECT &mldr;”写成“SLECT &mldr;”，查询就G了。</p><blockquote><p>But this goes deeper. It also checks that the keywords are used in the right order. For example a WHERE before a SELECT will be rejected.</p></blockquote><p>除此之外，解析器还会检查关键字的顺序。如果把 WHERE 写在 SELECT 之前这个请求同样会被拒绝。</p><blockquote><p>Then, the tables and the fields inside the query are analyzed. The parser uses the metadata of the database to check:</p><ul><li>If the tables exist</li><li>If the fields of the tables exist</li><li>If the operations for the types of the fields are possible (for example you can’t compare an integer with a string, you can’t use a substring() function on an integer)</li></ul></blockquote><p>随后被访问的表和相应字段将被分析。解析器通过使用数据库存储的元数据来进行如下检查：</p><ul><li>表是否存在</li><li>请求中访问的字段在表中是否存在</li><li>对表中字段的操作是否符合类型约束(例如不能将字符串和整数进行比较，不能将substring()用在整数上)</li></ul><blockquote><p>Then it checks if you have the authorizations to read (or write) the tables in the query. Again, these access rights on tables are set by your DBA.</p></blockquote><p>随后解析器检查用户是否有查询中表的 <strong>操作授权</strong> 。访问权限是由DBA设置的。</p><blockquote><p>During this parsing, the SQL query is transformed into an internal representation (often a tree)</p></blockquote><p>在解析过程中，一条SQL语句会被转化成一种内部表示(通常是一棵树)</p><blockquote><p>If everything is ok then the internal representation is sent to the query rewriter.</p></blockquote><p>如果这些检查都通过了，内部表示将会被送到查询重写器。</p><h2 id=42-查询重写>4.2 查询重写</h2><blockquote><p>At this step, we have an internal representation of a query. The aim of the rewriter is:</p><ul><li>to pre-optimize the query</li><li>to avoid unnecessary operations</li><li>to help the optimizer to find the best possible solution</li></ul></blockquote><p>到这一步，我们有的是查询的内部表示。重写器的目标如下：</p><ul><li>对查询进行预优化</li><li>避免不必要的操作</li><li>帮优化器找到可能的最优解</li></ul><blockquote><p>The rewriter executes a list of known rules on the query. If the query fits a pattern of a rule, the rule is applied and the query is rewritten. Here is a non-exhaustive list of (optional) rules:</p><ul><li>View merging: If you’re using a view in your query, the view is transformed with the SQL code of the view.</li><li>Subquery flattening: Having subqueries is very difficult to optimize so the rewriter will try to modify a query with a subquery to remove the subquery.</li></ul></blockquote><p>重写器对查询按照一系列知名的规则对查询执行重写。
如果查询符合某一种规则的模式，就说这个规则已经生效并且查询已完成重写。
列举几个不详尽的改写规则：</p><ul><li><strong>视图合并</strong>：如果在查询中使用了视图，视图将被转换成其SQL代码</li><li><strong>子查询展开</strong>：对有子查询的查询进行优化很难，因此重写器会尝试重写查询将查询中的子查询打平</li></ul><blockquote><p>For example</p></blockquote><div class=highlight><pre tabindex=0 class=chroma><code class=language-SQL data-lang=SQL><span class=line><span class=cl><span class=k>SELECT</span><span class=w> </span><span class=n>PERSON</span><span class=p>.</span><span class=o>*</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=k>FROM</span><span class=w> </span><span class=n>PERSON</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=k>WHERE</span><span class=w> </span><span class=n>PERSON</span><span class=p>.</span><span class=n>person_key</span><span class=w> </span><span class=k>IN</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=p>(</span><span class=k>SELECT</span><span class=w> </span><span class=n>MAILS</span><span class=p>.</span><span class=n>person_key</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=k>FROM</span><span class=w> </span><span class=n>MAILS</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=k>WHERE</span><span class=w> </span><span class=n>MAILS</span><span class=p>.</span><span class=n>mail</span><span class=w> </span><span class=k>LIKE</span><span class=w> </span><span class=s1>&#39;christophe%&#39;</span><span class=p>);</span><span class=w>
</span></span></span></code></pre></div><blockquote><p>Will be replaced by</p></blockquote><div class=highlight><pre tabindex=0 class=chroma><code class=language-SQL data-lang=SQL><span class=line><span class=cl><span class=k>SELECT</span><span class=w> </span><span class=n>PERSON</span><span class=p>.</span><span class=o>*</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=k>FROM</span><span class=w> </span><span class=n>PERSON</span><span class=p>,</span><span class=w> </span><span class=n>MAILS</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=k>WHERE</span><span class=w> </span><span class=n>PERSON</span><span class=p>.</span><span class=n>person_key</span><span class=w> </span><span class=o>=</span><span class=w> </span><span class=n>MAILS</span><span class=p>.</span><span class=n>person_key</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=k>and</span><span class=w> </span><span class=n>MAILS</span><span class=p>.</span><span class=n>mail</span><span class=w> </span><span class=k>LIKE</span><span class=w> </span><span class=s1>&#39;christophe%&#39;</span><span class=p>;</span><span class=w>
</span></span></span></code></pre></div><blockquote><ul><li>Removal of unnecessary operators: For example if you use a DISTINCT whereas you have a UNIQUE constraint that prevents the data from being non-unique, the DISTINCT keyword is removed.</li><li>Redundant join elimination: If you have twice the same join condition because one join condition is hidden in a view or if by transitivity there is a useless join, it’s removed.</li><li>Constant arithmetic evaluation: If you write something that requires a calculus, then it’s computed once during the rewriting. For example WHERE AGE > 10+2 is transformed into WHERE AGE > 12 and TODATE(“some date”) is transformed into the date in the datetime format</li><li>(Advanced) Partition Pruning: If you’re using a partitioned table, the rewriter is able to find what partitions to use.</li><li>(Advanced) Materialized view rewrite: If you have a materialized view that matches a subset of the predicates in your query, the rewriter checks if the view is up to date and modifies the query to use the materialized view instead of the raw tables.</li><li>(Advanced) Custom rules: If you have custom rules to modify a query (like Oracle policies), then the rewriter executes these rules</li><li>(Advanced) Olap transformations: analytical/windowing functions, star joins, rollup … are also transformed (but I’m not sure if it’s done by the rewriter or the optimizer, since both processes are very close it must depends on the database).</li></ul></blockquote><ul><li><strong>移除多余操作</strong>：例如你在一张已经加上UNIQUE限定的表中进行DISTINCT查询时，DISTINCT会被移除，因为表中的数据不可能出现重复</li><li><strong>消除冗余的联结</strong>：因一个联结条件隐藏在视图中或因传递性推导出无用的联结而产生的两次相同的联结条件，多余的联结条件会被移除。</li><li><strong>常量计算替换</strong>：如果在查询中有常量运算会在重写阶段直接替换成结果。例如<code>WHERE AGE>10+2</code>会被转换成<code>WHERE AGE > 12</code>，与此类似的还有更新时间操作。</li><li><strong>(进阶的)分片剪枝</strong>：如果是分片表，重写器能够从SQL中推测出需要访问哪些分片。<em>从而减少分片的访问次数</em></li><li><strong>(进阶的)物化视图重写</strong>：如果此时有一张匹配了查询谓词子集的<a href=https://en.wikipedia.org/wiki/Materialized_view target=_blank rel="noopener noreffer">物化视图</a>，重写器会检查视图的时效性并调整查询使用该视图的数据，而不是从原始表中取</li><li><strong>(进阶的)自定义的规则</strong>：如果你有一些自定义对查询改写的规则(例如Oracle policies)，重写器会执行这些规则</li><li><strong>(进阶的)OLAP转换</strong>：分析型/窗口函数、星型联结、滚动等等也将被转换(但我不确定这一步究竟是由重写器还是优化器做的，因为这两步处理都很相似，必须取决于实际的数据库)</li></ul><blockquote><p>This rewritten query is then sent to the query optimizer where the fun begins!</p></blockquote><p>重写的查询随后送至查询优化器，至此有意思的部分就要开始了！</p><h2 id=43-统计信息>4.3 统计信息</h2><blockquote><p>Before we see how a database optimizes a query we need to speak about statistics because without them a database is stupid. If you don’t tell the database to analyze its own data, it will not do it and it will make (very) bad assumptions.</p></blockquote><p>在了解数据库如何对查询进行优化之前我们需要聊聊 <strong>统计数据</strong> ，如果<strong>没有这些信息数据库会显得很蠢</strong>。
如果你不让数据库分析其存储的数据，它便不会分析，并且会作出一些很捞的假设。</p><blockquote><p>But what kind of information does a database need?</p></blockquote><p>数据库需要什么样的信息呢？</p><blockquote><p>I have to (briefly) talk about how databases and Operating systems store data. They’re using a minimum unit called a page or a block (4 or 8 kilobytes by default). This means that if you only need 1 Kbytes it will cost you one page anyway. If the page takes 8 Kbytes then you’ll waste 7 Kbytes.</p></blockquote><p>我将简要讨论一下数据库和操作系统如何存储数据的。
它们都使用一个最小的存储单元：<strong>页/块</strong>(通常4kb或8kb)。
意思是即使你只需要1kb的数据，也需要将一整页的数据读入内存。
如果页是8kb，相当于这次操作浪费了7kb的读入数据。</p><blockquote><p>Back to the statistics! When you ask a database to gather statistics, it computes values like:</p><ul><li>The number of rows/pages in a table</li><li>For each column in a table:<ul><li>distinct data values</li><li>the length of data values (min, max, average)</li><li>data range information (min, max, average)</li></ul></li><li>Information on the indexes of the table.</li></ul></blockquote><p>说回统计数据。当你告诉数据库去收集统计数据的时候，它会统计如下数据：</p><ul><li>表的行数/页数</li><li>对于表中的每一列：<ul><li>不同数据的个数</li><li>数据的长度(最大值、最小值、平均值)</li><li>数据范围信息(最大值、最小值、平均值)</li></ul></li><li>表的索引的信息</li></ul><blockquote><p>These statistics will help the optimizer to estimate the disk I/O, CPU and memory usages of the query.</p></blockquote><p><strong>这些统计信息能够帮助优化器来评估一次查询的磁盘IO，CPU以及内存的使用情况。</strong></p><blockquote><p>The statistics for each column are very important. For example if a table PERSON needs to be joined on 2 columns: LAST_NAME, FIRST_NAME. With the statistics, the database knows that there are only 1 000 different values on FIRST_NAME and 1 000 000 different values on LAST_NAME. Therefore, the database will join the data on LAST_NAME, FIRST_NAME instead of FIRST_NAME,LAST_NAME because it produces way less comparisons since the LAST_NAME are unlikely to be the same so most of the time a comparison on the 2 (or 3) first characters of the LAST_NAME is enough.</p></blockquote><p>对每行的统计数据很重要。例如一张PERSON表需要对两列做联结：姓，名。
在有统计数据时，数据库知道名只有1 000个不同值，而姓有1 000 000个不同值。
因此数据库会选择按照姓，名的顺序进行联结而不是相反的顺序，因为这样能够进行更少的比较操作，因为对姓可能进行两三个字符的比较就能够得到顺序。</p><blockquote><p>But these are basic statistics. You can ask a database to compute advanced statistics called histograms. Histograms are statistics that inform about the distribution of the values inside the columns. For example</p><ul><li>the most frequent values</li><li>the quantiles</li><li>…</li></ul></blockquote><p>但这些仅是基础的统计数据。你可以要求数据库计算一些进阶的数据如 <strong>直方图</strong>。直方图能感知列中数据的分布。例如：</p><ul><li>出现频率最高的值</li><li>分位值</li><li>&mldr;</li></ul><blockquote><p>These extra statistics will help the database to find an even better query plan. Especially for equality predicate (ex: WHERE AGE = 18 ) or range predicates (ex: WHERE AGE > 10 and AGE &lt;40 ) because the database will have a better idea of the number rows concerned by these predicates (note: the technical word for this concept is selectivity).</p></blockquote><p>这些额外的统计数据能够帮助数据库找到更好的查询计划。
尤其是对于相等谓词(例如 <code>WHERE AGE = 18</code>)或者是范围谓词(例如 <code>WHERE AGE > 10 and AGE &lt; 40</code>)，因为数据库对这些谓词涉及的行数有更清晰的认知(注：这个概念的技术术语是选择性)</p><blockquote><p>The statistics are stored in the metadata of the database. For example you can see the statistics for the (non-partitioned) tables:</p><ul><li>in USER/ALL/DBA_TABLES and USER/ALL/DBA_TAB_COLUMNS for Oracle</li><li>in SYSCAT.TABLES and SYSCAT.COLUMNS for DB2.</li></ul></blockquote><p>统计数据存在数据库的元数据中。你可以在下面这些表中看到统计数据：</p><ul><li>Oracle：USER/ALL/DBA_TABLES 和 USER/ALL/DBA_TAB_COLUMNS</li><li>DB2：SYSCAT.TABLES 和 SYSCAT.COLUMNS</li></ul><blockquote><p>The statistics have to be up to date. There is nothing worse than a database thinking a table has only 500 rows whereas it has 1 000 000 rows. The only drawback of the statistics is that it takes time to compute them. This is why they’re not automatically computed by default in most databases. It becomes difficult with millions of data to compute them. In this case, you can choose to compute only the basics statistics or to compute the stats on a sample of the database.</p></blockquote><p><strong>统计数据需要及时更新</strong>。
没什么比数据库认为一张表只有500行而实际由1 000 000 行更糟的事情了。
统计数据的唯一缺点是其需要额外的时间来计算相应值。
这也是为什么它们在大多数数据库中并不是一项默认开启的功能。
当数据量到达百万级别的时候想计算出他们还挺难的。
在这种情况下你可以选择只计算出基础的统计数据或者利用数据库的采样来估算相应统计数据。</p><blockquote><p>For example, when I was working on a project dealing with hundreds of millions rows in each tables, I chose to compute the statistics on only 10%, which led to a huge gain in time. For the story it turned out to be a bad decision because occasionally the 10% chosen by Oracle 10G for a specific column of a specific table were very different from the overall 100% (which is very unlikely to happen for a table with 100M rows). This wrong statistic led to a query taking occasionally 8 hours instead of 30 seconds; a nightmare to find the root cause. This example shows how important the statistics are.</p></blockquote><p>举个例子，当我在一个每张表需要处理上十亿行数据的项目干活的时候，我选择依据10%的数据来计算统计数据，在计算时间上确实有巨大提升。
但故事的结果表明这个选择有大问题，因为Oracle 10G随机选的10%列对于这样一张表来说与全量数据的特征相差很大(这对于一张有着十亿级别数据的表来说发生概率是很低的)。
错误的统计数据导致查询有时候需要花8小时而不是30秒；想找到问题的根因简直是噩梦。
这个例子表明统计数据有多重要。</p><blockquote><p>Note: Of course, there are more advanced statistics specific for each database. If you want to know more, read the documentations of the databases. That being said, I’ve tried to understand how the statistics are used and the best official documentation I found was the one from PostgreSQL.</p></blockquote><p>注：当然，对于不同的数据库来说还有许多进阶的统计数据。如果你想了解更多，可以读一下数据库的文档。
话虽如此，我觉得写得最好的官方文档是<a href=https://www.postgresql.org/docs/9.4/static/row-estimation-examples.html target=_blank rel="noopener noreffer">PostgreSQL的这篇</a>.</p><h2 id=44-查询优化器>4.4 查询优化器</h2><blockquote><p>All modern databases are using a Cost Based Optimization (or CBO) to optimize queries. The idea is to put a cost an every operation and find the best way to reduce the cost of the query by using the cheapest chain of operations to get the result.</p></blockquote><p>所有现代数据库都使用一种叫做 <strong>基于开销的优化方法</strong> 来优化查询。
大体思路是通过给每种操作标上一个开销值，然后通过使用开销最小的操作链去取结果来寻找一种减少查询开销值的最好的方式。</p><blockquote><p>To understand how a cost optimizer works I think it’s good to have an example to “feel” the complexity behind this task. In this part I’ll present you the 3 common ways to join 2 tables and we will quickly see that even a simple join query is a nightmare to optimize. After that, we’ll see how real optimizers do this job.</p></blockquote><p>为了理解开销优化器如何工作，最好的方式是举个例子来切身感受这个任务背后的复杂度。
在这个部分我会向你展示三种常见的用于联结两表的方式，并且我们会马上发现，即使是想要优化最简单的联结查询难度如同噩梦。
在这之后，我们将看看优化器具体是怎么做的。</p><blockquote><p>For these joins, I’ll focus on their time complexity but a database optimizer computes their CPU cost, disk I/O cost and memory requirement. The difference between time complexity and CPU cost is that time cost is very approximate (it’s for lazy guys like me). For the CPU cost, I should count every operation like an addition, an “if statement”, a multiplication, an iteration … Moreover:</p><ul><li>Each high level code operation has a specific number of low level CPU operations.</li><li>The cost of a CPU operation is not the same (in terms of CPU cycles) whether you’re using an Intel Core i7, an Intel Pentium 4, an AMD Opteron…. In other words it depends on the CPU architecture.</li></ul></blockquote><p>对于这些联结，我会聚焦于他们的时间复杂度，但是 <strong>数据库优化器</strong> 更关心的是它们的 <strong>CPU开销、磁盘IO开销以及内存需求</strong>。
时间复杂度和CPU开销的区别在于时间复杂度只是一个近似值(它是给我们这些懒逼用的)。
而CPU开销需要将每一步操作带来的开销累加起来，例如一次加法、一次条件判断、一次乘法、一次循环&mldr;
而且：</p><ul><li>每一个上层代码的操作有着不同的下层CPU开销数量</li><li>CPU的开销在不同的CPU架构上也不尽相同</li></ul><blockquote><p>Using the time complexity is easier (at least for me) and with it we can still get the concept of CBO. I’ll sometimes speak about disk I/O since it’s an important concept. Keep in mind that the bottleneck is most of the time the disk I/O and not the CPU usage.</p></blockquote><p>使用时间复杂度更简单，并且也能得到CBO的大体情况。
我有时会提到磁盘IO，因为它很重要。
需要记住的是大部分时候瓶颈都是磁盘IO而不是CPU占用。</p><h3 id=441-索引>4.4.1 索引</h3><blockquote><p>We talked about indexes when we saw the B+Trees. Just remember that these indexes are already sorted.</p></blockquote><p>在之前介绍B+树的时候提到了索引。
只需记住这些索引是 <strong>有序的</strong>。</p><blockquote><p>FYI, there are other types of indexes like bitmap indexes. They don’t offer the same cost in terms of CPU, disk I/O and memory than B+Tree indexes.</p></blockquote><p>供你参考，还有其他索引类型，例如位图索引。
他们在CPU、磁盘IO以及内存的开销与B+树索引不同。</p><blockquote><p>Moreover, many modern databases can dynamically create temporary indexes just for the current query if it can improve the cost of the execution plan.</p></blockquote><p>此外，许多现代数据库能够为当前的查询 <strong>动态创建临时索引</strong>，如果这种方式能够优化执行计划。</p><h3 id=442-访问路径>4.4.2 访问路径</h3><blockquote><p>Before applying your join operators, you first need to get your data. Here is how you can get your data.</p><p>Note: Since the real problem with all the access paths is the disk I/O, I won’t talk a lot about time complexity.</p></blockquote><p>在真正进行联结操作之前，首先需要取到数据，接下来介绍怎么取数据。</p><p>注：因为在取数据这一环主要的开销是磁盘IO，所以不会太谈到时间复杂度。</p><blockquote><p>Full scan</p><p>If you’ve ever read an execution plan you must have seen the word full scan (or just scan). A full scan is simply the database reading a table or an index entirely. In terms of disk I/O, a table full scan is obviously more expensive than an index full scan.</p></blockquote><p><strong>全扫描</strong></p><p>如果你读到过关于执行计划的内容，你应该看过 <strong>全扫描</strong> 这个词。全扫描指数据库完整地读一整张表或一整个索引。
全表扫描的磁盘开销肯定是比全索引扫描高。</p><blockquote><p>Range Scan</p><p>There are other types of scan like index range scan. It is used for example when you use a predicate like “WHERE AGE > 20 AND AGE &lt;40”.</p></blockquote><p><strong>范围扫描</strong></p><p>还有其他的扫描类型，例如<strong>对索引的范围扫描</strong>。
当你写出<code>WHERE AGE > 20 AND AGE &lt; 40</code>这样的语句时执行的就是范围扫描。</p><blockquote><p>Of course you need have an index on the field AGE to use this index range scan.</p></blockquote><p>但是你需要在<code>AGE</code>上建立了索引才能使用范围扫描。</p><blockquote><p>We already saw in the first part that the time cost of a range query is something like log(N) +M, where N is the number of data in this index and M an estimation of the number of rows inside this range. Both N and M values are known thanks to the statistics (Note: M is the selectivity for the predicate AGE >20 AND AGE&lt;40). Moreover, for a range scan you don’t need to read the full index so it’s less expensive in terms of disk I/O than a full scan.</p></blockquote><p>我们在第一章已经说过，范围查询的开销近似为$log(N)+M$，N是索引的数据量，M是范围中行的估计值。
<strong>因为有统计数据，N和M都是已知的。</strong>
除此之外，对于范围扫描来说你无需扫描完整个索引，因此<strong>磁盘开销比全量扫描低</strong>。</p><blockquote><p>Unique scan</p><p>If you only need one value from an index you can use the unique scan.</p></blockquote><p><strong>单点扫描</strong></p><p>如果你只需要索引中的一个值可以使用单点扫描。</p><blockquote><p>Access by row id</p><p>Most of the time, if the database uses an index, it will have to look for the rows associated to the index. To do so it will use an access by row id.</p></blockquote><p><strong>通过行id访问</strong></p><p>大多数数据库使用索引的时候，都会查看与索引相关联的行。
想要实现这一点需要通过行id访问数据。</p><blockquote><p>For example, if you do something like</p><p>If you have an index for person on column age, the optimizer will use the index to find all the persons who are 28 then it will ask for the associate rows in the table because the index only has information about the age and you want to know the lastname and the firstname.</p></blockquote><p>例如你可能写出这样的SQL：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-SQL data-lang=SQL><span class=line><span class=cl><span class=k>SELECT</span><span class=w> </span><span class=n>LASTNAME</span><span class=p>,</span><span class=w> </span><span class=n>FIRSTNAME</span><span class=w> </span><span class=k>from</span><span class=w> </span><span class=n>PERSON</span><span class=w> </span><span class=k>WHERE</span><span class=w> </span><span class=n>AGE</span><span class=w> </span><span class=o>=</span><span class=w> </span><span class=mi>28</span><span class=w>
</span></span></span></code></pre></div><p>如果在person表中有对age的索引，那么优化器将使用索引来找到所有age=28的人的行id，然后通过行id来访问到你所需要的lastname和firstname。
之所以需要这么一步间接访问是因为索引中只存了行id，没有存行内信息。</p><blockquote><p>But, if now you do something like</p><p>The index on PERSON will be used to join with TYPE_PERSON but the table PERSON will not be accessed by row id since you’re not asking information on this table.</p></blockquote><p>但是如果你写出如下SQL：</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-SQL data-lang=SQL><span class=line><span class=cl><span class=k>SELECT</span><span class=w> </span><span class=n>TYPE_PERSON</span><span class=p>.</span><span class=n>CATEGORY</span><span class=w> </span><span class=k>from</span><span class=w> </span><span class=n>PERSON</span><span class=w> </span><span class=p>,</span><span class=n>TYPE_PERSON</span><span class=w>
</span></span></span><span class=line><span class=cl><span class=w></span><span class=k>WHERE</span><span class=w> </span><span class=n>PERSON</span><span class=p>.</span><span class=n>AGE</span><span class=w> </span><span class=o>=</span><span class=w> </span><span class=n>TYPE_PERSON</span><span class=p>.</span><span class=n>AGE</span><span class=w>
</span></span></span></code></pre></div><p>会使用person表的索引来与type_person表做联结，但是不会对person表进行访问，因为没有使用到表内的信息。</p><blockquote><p>Though it works great for a few accesses, the real issue with this operation is the disk I/O. If you need too many accesses by row id the database might choose a full scan.</p></blockquote><p>尽管这种方式对于少量访问来说运行得很好，但真正的问题在于磁盘IO。
如果需要进行大量的通过行id的访问，数据库可能会选择使用全量扫描。</p><blockquote><p>Others paths</p><p>I didn’t present all the access paths. If you want to know more, you can read the Oracle documentation. The names might not be the same for the other databases but the concepts behind are the same.</p></blockquote><p><strong>其他路径</strong></p><p>这里并没有列举出所有的访问路径，如果你想了解更多内容，可以读一下这篇<a href=https://docs.oracle.com/database/121/TGSQL/tgsql_optop.htm target=_blank rel="noopener noreffer">Oracle的文档</a>.
不同的数据库对这些访问路径的命名可能不同，但背后的思想都是一样的。</p><h3 id=443-联结操作>4.4.3 联结操作</h3><blockquote><p>So, we know how to get our data, let’s join them!</p></blockquote><p>知道怎么取数据后开联！</p><blockquote><p>I’ll present the 3 common join operators: Merge Join, Hash Join and Nested Loop Join. But before that, I need to introduce new vocabulary: inner relation and outer relation. A relation can be:</p><ul><li>a table</li><li>an index</li><li>an intermediate result from a previous operation (for example the result of a previous join)</li></ul></blockquote><p>我会展示三种常见的联结操作：合并联结、哈希联结、嵌套循环联结。
在那之前需要介绍一下新名词：<strong>内联表</strong> 和 <strong>外联表</strong>，表可以是：</p><ul><li>表</li><li>索引</li><li>前一个操作的中间结果(例如前一个联结的结果)</li></ul><blockquote><p>When you’re joining two relations, the join algorithms manage the two relations differently. In the rest of the article, I’ll assume that:</p><ul><li>the outer relation is the left data set</li><li>the inner relation is the right data set</li></ul></blockquote><p>当你联结两个表时，联结算法对于不同的表(内外)处理方式不同。
在本文的剩余部分会有如下假设：</p><ul><li>外联表指的是左边的数据集</li><li>内联表指的是右边的数据集</li></ul><blockquote><p>For example, A JOIN B is the join between A and B where A is the outer relation and B the inner relation.</p></blockquote><p>例如<code>A JOIN B</code>是AB之间的联结，这个联结中，A是外联表，B是内联表。</p><blockquote><p>Most of the time, the cost of A JOIN B is not the same as the cost of B JOIN A.</p></blockquote><p>在大多数时候，<strong><code>A JOIN B</code>和<code>B JOIN A</code>的开销不同</strong>。</p><blockquote><p>In this part, I’ll also assume that the outer relation has N elements and the inner relation M elements. Keep in mind that a real optimizer knows the values of N and M with the statistics.</p></blockquote><p>在这一章会假定<strong>外联表有N个元素，内联表有M个元素</strong>。
需要留意的是，真正的优化器是通过统计数据知道N和M的。</p><blockquote><p>Note: N and M are the cardinalities of the relations.</p></blockquote><p>注：N和M是表的基数。</p><blockquote><p>Nested loop join</p></blockquote><p><strong>嵌套循环联结</strong></p><blockquote><p>The nested loop join is the easiest one.</p></blockquote><p>嵌套循环联结是最简单的一个。</p><figure><a class=lightgallery href=/images/nested_loop_join.png title=/images/nested_loop_join.png data-thumbnail=/images/nested_loop_join.png data-sub-html="<h2>嵌套循环联结</h2>"><img class=lazyload src=/svg/loading.min.svg data-src=/images/nested_loop_join.png data-srcset="/images/nested_loop_join.png, /images/nested_loop_join.png 1.5x, /images/nested_loop_join.png 2x" data-sizes=auto alt=/images/nested_loop_join.png height=1500 width=1500></a><figcaption class=image-caption>嵌套循环联结</figcaption></figure><blockquote><p>Here is the idea:</p><ul><li>for each row in the outer relation</li><li>you look at all the rows in the inner relation to see if there are rows that match</li></ul></blockquote><p>大体思路如下：</p><ul><li>对外联表中的每一行</li><li>到内联表中查看是否有匹配的行</li></ul><p><em>可能两种表的命名方式也是来自这里，二者看起来像二重循环中的内循环(inner relation)和外循环(outer relation)</em></p><blockquote><p>Here is a pseudo code:</p></blockquote><p>伪代码如下：</p><pre tabindex=0><code>nested_loop_join(array outer, array inner)
  for each row a in outer
    for each row b in inner
      if (match_join_condition(a,b))
        write_result_in_output(a,b)
      end if
    end for
   end for
</code></pre><blockquote><p>Since it’s a double iteration, the time complexity is O(N*M)</p></blockquote><p>因为是二重循环，<strong>时间复杂度是$O(N*M)$</strong>。</p><blockquote><p>In term of disk I/O, for each of the N rows in the outer relation, the inner loop needs to read M rows from the inner relation. This algorithm needs to read N + N*M rows from disk. But, if the inner relation is small enough, you can put the relation in memory and just have M +N reads. With this modification, the inner relation must be the smallest one since it has more chance to fit in memory.</p></blockquote><p>看看磁盘IO开销，，对于外联表中N行中的每一行，内联循环需要从内联表中读出M行。
这个算法需要从磁盘中读出 $N+N*M$ 行。
但如果内联表足够小，以至于可以将其全部塞入内存的话，总共只需要读 $N+M$ 行。
在这种调整中，<strong>内联表必须足够小</strong>，这样更容易塞入内存。</p><blockquote><p>In terms of time complexity it makes no difference but in terms of disk I/O it’s way better to read only once both relations.</p></blockquote><p>时间复杂度虽然没有区别，但磁盘IO开销更小，因为只需要对两个表各读一次。</p><blockquote><p>Of course, the inner relation can be replaced by an index, it will be better for the disk I/O.</p></blockquote><p>内联表当然也可以被索引替换，这样磁盘IO开销更小。</p><blockquote><p>Since this algorithm is very simple, here is another version that is more disk I/O friendly if the inner relation is too big to fit in memory. Here is the idea:</p><ul><li>instead of reading both relation row by row,</li><li>you read them bunch by bunch and keep 2 bunches of rows (from each relation) in memory,</li><li>you compare the rows inside the two bunches and keep the rows that match,</li><li>then you load new bunches from disk and compare them</li><li>and so on until there are no bunches to load.</li></ul></blockquote><p>因为这种算法很简单，如果内联表大到没法塞入内存，还有一种对磁盘IO更友好的版本.
思路如下：</p><ul><li>与其逐行读取</li><li>可以批量读取，每次把这两批放在内存中</li><li>然后在批之前进行行比较，并把匹配的行留下</li><li>继续从磁盘读入新一批数据并进行比较</li><li>重复直到所有数据都比较完毕</li></ul><blockquote><p>Here is a possible algorithm:</p></blockquote><p>一种可能的实现：</p><pre tabindex=0><code>// improved version to reduce the disk I/O.
nested_loop_join_v2(file outer, file inner)
  for each bunch ba in outer
  // ba is now in memory
    for each bunch bb in inner
        // bb is now in memory
        for each row a in ba
          for each row b in bb
            if (match_join_condition(a,b))
              write_result_in_output(a,b)
            end if
          end for
       end for
    end for
   end for
</code></pre><blockquote><p>With this version, the time complexity remains the same, but the number of disk access decreases:</p><ul><li>With the previous version, the algorithm needs N + N*M accesses (each access gets one row).</li><li>With this new version, the number of disk accesses becomes number_of_bunches_for(outer)+ number_of_ bunches_for(outer)* number_of_ bunches_for(inner).</li><li>If you increase the size of the bunch you reduce the number of disk accesses.</li></ul></blockquote><p><strong>在这个版本中，时间复杂度仍是相同的，但是磁盘访问次数减少了</strong>：</p><ul><li>在前一个版本中，需要对磁盘进行 $N+N*M$ 次访问(每次访问取一行)</li><li>在新版本中对磁盘的访问次数是 $number_of_bunches_for(outer)+ number_of_ bunches_for(outer)* number_of_ bunches_for(inner)$</li><li>如果增大每一批的大小能够显著减少磁盘访问次数</li></ul><blockquote><p>Note: Each disk access gathers more data than the previous algorithm but it doesn’t matter since they’re sequential accesses (the real issue with mechanical disks is the time to get the first data).</p></blockquote><p>注：每一次磁盘访问都比前一种算法取了更多数据，但并不会导致性能变差，因为这些访问都是连续的访问
（对机械键盘来说真正关键的开销是取第一个数据的时间)。</p><blockquote><p>Hash join</p><p>The hash join is more complicated but gives a better cost than a nested loop join in many situations.</p></blockquote><p><strong>哈希联结</strong></p><p>哈希联结更复杂一些，但是在很多情况下比嵌套循环联结的性能更好。</p><figure><a class=lightgallery href=/images/hash_join.png title=/images/hash_join.png data-thumbnail=/images/hash_join.png data-sub-html="<h2>哈希联结</h2>"><img class=lazyload src=/svg/loading.min.svg data-src=/images/hash_join.png data-srcset="/images/hash_join.png, /images/hash_join.png 1.5x, /images/hash_join.png 2x" data-sizes=auto alt=/images/hash_join.png height=1500 width=1500></a><figcaption class=image-caption>哈希联结</figcaption></figure><blockquote><p>The idea of the hash join is to:</p><ul><li><ol><li>Get all elements from the inner relation</li></ol></li><li><ol start=2><li>Build an in-memory hash table</li></ol></li><li><ol start=3><li>Get all elements of the outer relation one by one</li></ol></li><li><ol start=4><li>Compute the hash of each element (with the hash function of the hash table) to find the associated bucket of the inner relation</li></ol></li><li><ol start=5><li>find if there is a match between the elements in the bucket and the element of the outer table</li></ol></li></ul></blockquote><p>哈希联结的思路如下：</p><ul><li>取出内联表中的所有元素</li><li>在内存中构建一张哈希表</li><li>一个接一个地从外联表中取元素</li><li>计算每个元素的哈希值(用哈希表的哈希函数)来找到与之相关联的内联表的桶</li><li>看看桶中元素和外联表中元素是否有匹配项</li></ul><blockquote><p>In terms of time complexity I need to make some assumptions to simplify the problem:</p><ul><li>The inner relation is divided into X buckets</li><li>The hash function distributes hash values almost uniformly for both relations. In other words the buckets are equally sized.</li><li>The matching between an element of the outer relation and all elements inside a bucket costs the number of elements inside the buckets.</li></ul></blockquote><p>在探讨时间复杂度之前需要作出如下假设来简化问题：</p><ul><li>内联表被分到了X个桶中</li><li>哈希函数对两种表的打散效果都是均匀的。换言之，桶大小是都是相等的</li><li>一个外联表中的元素和桶中所有元素进行匹配的时间开销是桶中元素个数</li></ul><blockquote><p>The time complexity is (M/X) * N + cost_to_create_hash_table(M) + cost_of_hash_function*N</p></blockquote><p>时间复杂度是 $(M/X)<em>N+cost_to_create_hash_table(M) + cost_of_hash_function</em>N$</p><blockquote><p>If the Hash function creates enough small-sized buckets then the time complexity is O(M+N)</p></blockquote><p>如果哈希函数创建出来的桶的大小足够小，时间复杂度就是$O(M+N)$</p><blockquote><p>Here is another version of the hash join which is more memory friendly but less disk I/O friendly. This time:</p><ul><li><ol><li>you compute the hash tables for both the inner and outer relations</li></ol></li><li><ol start=2><li>then you put them on disk</li></ol></li><li><ol start=3><li>then you compare the 2 relations bucket by bucket (with one loaded in-memory and the other read row by row)</li></ol></li></ul></blockquote><p>再介绍一种内存友好型的哈希联结，这个版本对磁盘IO来说不那么友好。
这次：</p><ul><li>对内外表分别建一张哈希表</li><li>然后将其返回磁盘</li><li>然后逐桶比较二者</li></ul><div class="details admonition note open"><div class="details-summary admonition-title"><i class="icon fas fa-pencil-alt fa-fw"></i>注意<i class="details-icon fas fa-angle-right fa-fw"></i></div><div class=details-content><div class=admonition-content>关于哈希联结这一段我觉得原文介绍得太过粗糙，不仅第一眼没有看来时间复杂度怎么算的，也没看出来为什么哈希联结是work的。主要在于其介绍建哈希表的时候没有说明表的键是什么，这里表的键是需要进行联结的列，因此在比较阶段实际作出的操作是：把可能相同的列拿出来比一比看看是否真正相同，如果相同就加入结果集。</div></div></div><blockquote><p>Merge join</p><p>The merge join is the only join that produces a sorted result.</p></blockquote><p><strong>合并联结</strong></p><p><strong>合并联结是唯一生成有序结果的联结。</strong></p><blockquote><p>Note: In this simplified merge join, there are no inner or outer tables; they both play the same role. But real implementations make a difference, for example, when dealing with duplicates.</p></blockquote><p>注：在这个简化的合并联结中，没有内、外联表；二者角色相同。
但是真正的实现中二者是不同的，例如在处理有重复数据的时候。</p><blockquote><p>The merge join can be divided into of two steps:</p><ul><li>(Optional) Sort join operations: Both the inputs are sorted on the join key(s).</li><li>Merge join operation: The sorted inputs are merged together.</li></ul></blockquote><p>合并联结可以分为以下两个步骤：</p><ul><li>(可选的)对联结操作排序：将所有的输入按照联结的键进行排序。</li><li>合并联结操作：排序的输入一起合并。</li></ul><blockquote><p>Sort</p><p>We already spoke about the merge sort, in this case a merge sort in a good algorithm (but not the best if memory is not an issue).</p></blockquote><p>排序</p><p>在之前已经介绍过归并排序，在这种情况下归并排序是一种不错的算法(但如果内存足够的话还有更好的)</p><blockquote><p>But sometimes the data sets are already sorted, for example:</p><ul><li>If the table is natively ordered, for example an index-organized table on the join condition</li><li>If the relation is an index on the join condition</li><li>If this join is applied on an intermediate result already sorted during the process of the query</li></ul></blockquote><p>有些时候数据集已经是有序的了，例如：</p><ul><li>如果表是天然有序的，例如对索引组织表进行联结</li><li>如果联结条件中的表是索引</li><li>如果对查询中已经排好序的中间结果进行联结</li></ul><p>合并联结</p><figure><a class=lightgallery href=/images/merge_join.png title=/images/merge_join.png data-thumbnail=/images/merge_join.png data-sub-html="<h2>合并联结</h2>"><img class=lazyload src=/svg/loading.min.svg data-src=/images/merge_join.png data-srcset="/images/merge_join.png, /images/merge_join.png 1.5x, /images/merge_join.png 2x" data-sizes=auto alt=/images/merge_join.png height=1500 width=1500></a><figcaption class=image-caption>合并联结</figcaption></figure><blockquote><p>This part is very similar to the merge operation of the merge sort we saw. But this time, instead of picking every element from both relations, we only pick the elements from both relations that are equals. Here is the idea:</p><ul><li><ol><li>you compare both current elements in the 2 relations (current=first for the first time)</li></ol></li><li><ol start=2><li>if they’re equal, then you put both elements in the result and you go to the next element for both relations</li></ol></li><li><ol start=3><li>if not, you go to the next element for the relation with the lowest element (because the next element might match)</li></ol></li><li><ol start=4><li>and repeat 1,2,3 until you reach the last element of one of the relation.</li></ol></li></ul></blockquote><p>这一部分和在归并排序中看到的合并缓解很相似。
但这一次只从两种表中取出相等的。
大体思路如下：</p><ol><li>对两种表进行比较</li><li>如果相同，将其塞入结果集，并移动两个指针</li><li>如果不同，将更小的那个指针向后移动</li><li>重复1,2,3直到某一个表中的指针移动到最后</li></ol><blockquote><p>This works because both relations are sorted and therefore you don’t need to “go back” in these relations.</p></blockquote><p>之所以能work是因为做合并的二者都是有序的，所以无需在其中回溯。</p><blockquote><p>This algorithm is a simplified version because it doesn’t handle the case where the same data appears multiple times in both arrays (in other words a multiple matches). The real version is more complicated “just” for this case; this is why I chose a simplified version.</p></blockquote><p>介绍的算法是简化版本，因为它并没有处理出现重复数据的情况。
真正的实现更加复杂。</p><blockquote><p>If both relations are already sorted then the time complexity is O(N+M)</p></blockquote><p>如果两个做联结的表都是有序的时间复杂度是$O(N+M)$。</p><blockquote><p>If both relations need to be sorted then the time complexity is the cost to sort both relations: O(N<em>Log(N) + M</em>Log(M))</p></blockquote><p>如果所有表都需要进行排序操作那么时间复杂度是$O(N<em>Log(N)+M</em>Log(M))$.</p><blockquote><p>For the CS geeks, here is a possible algorithm that handles the multiple matches (note: I’m not 100% sure about my algorithm):</p></blockquote><p>列一个我认为可行的处理有重复数据的归并联结：</p><pre tabindex=0><code>mergeJoin(relation a, relation b)
  relation output
  integer a_key:=0;
  integer b_key:=0;
  
  while (a[a_key]!=null or b[b_key]!=null)
    if (a[a_key] &lt; b[b_key])
      a_key++;
    else if (a[a_key] &gt; b[b_key])
      b_key++;
    else //Join predicate satisfied
    //i.e. a[a_key] == b[b_key]
 
      //count the number of duplicates in relation a
      integer nb_dup_in_a = 1:
      while (a[a_key]==a[a_key+nb_dup_in_a])
        nb_dup_in_a++;
         
      //count the number of duplicates in relation b
      integer dup_in_b = 1:
      while (b[b_key]==b[b_key+nb_dup_in_b])
        nb_dup_in_b++;
         
      //write the duplicates in output
       for (int i = 0 ; i&lt; nb_dup_in_a ; i++)
         for (int j = 0 ; i&lt; nb_dup_in_b ; i++)     
           write_result_in_output(a[a_key+i],b[b_key+j])
            
      a_key=a_key + nb_dup_in_a-1;
      b_key=b_key + nb_dup_in_b-1;
 
    end if
  end while
</code></pre><blockquote><p>Which one is the best?</p><p>If there was a best type of joins, there wouldn’t be multiple types. This question is very difficult because many factors come into play like:</p><ul><li>The amount of free memory: without enough memory you can say goodbye to the powerful hash join (at least the full in-memory hash join)</li><li>The size of the 2 data sets. For example if you have a big table with a very small one, a nested loop join will be faster than a hash join because the hash join has an expensive creation of hashes. If you have 2 very large tables the nested loop join will be very CPU expensive.</li><li>The presence of indexes. With 2 B+Tree indexes the smart choice seems to be the merge join</li><li>If the result need to be sorted: Even if you’re working with unsorted data sets, you might want to use a costly merge join (with the sorts) because at the end the result will be sorted and you’ll be able to chain the result with another merge join (or maybe because the query asks implicitly/explicitly for a sorted result with an ORDER BY/GROUP BY/DISTINCT operation)</li><li>If the relations are already sorted: In this case the merge join is the best candidate</li><li>The type of joins you’re doing: is it an equijoin (i.e.: tableA.col1 = tableB.col2)? Is it an inner join, an outer join, a cartesian product or a self-join? Some joins can’t work in certain situations.</li><li>The distribution of data. If the data on the join condition are skewed (For example you’re joining people on their last name but many people have the same), using a hash join will be a disaster because the hash function will create ill-distributed buckets.</li><li>If you want the join to be executed by multiple threads/process</li></ul></blockquote><p>哪种最好？</p><p>如果有最好的联结的话就不会出来这么多种了。
这个问题涉及的因素很多：</p><ul><li><strong>空余内存大小</strong>：如果内存空间不足的话就可以和哈希联结说拜拜了(至少没法用全内存的哈希联结)</li><li><strong>两个数据集的大小</strong>：如果你要对一张大表和一张很小的表做联结，嵌套循环联结比哈希联结快得多，因为哈希联结创建哈希开销很大。如果两种表都很大，使用嵌套循环联结CPU的开销会很大。</li><li><strong>是否使用索引</strong>：如果在两个B+树索引之前进行联结，合并联结优选。</li><li><strong>结果是否需要排序</strong>：即使当前数据集都是无序的，你也可能想使用开销很大的合并联结因为在最后想要得到有序的结果，从而可以与其他的合并联结进行链式操作(或仅仅是因为查询中使用类似ORDER BY/GROUP BY/DISTINCT这样的操作)</li><li><strong>表是否已经排过序</strong>：如果已经有序，使用合并联结是优选。</li><li>联结的类型：等值联结？内联结？外联结？笛卡尔积？还是半联结？有的联结无法用在特定的情况中。</li><li><strong>数据的分布性</strong>。如果在联结条件中的数据是<strong>倾斜</strong>的(例如你对姓进行联结，此时很多人的姓是相同的)，此时使用哈希联结会很痛，因为桶是非常不均匀的。</li><li>是否希望联结操作<strong>多线/进程</strong>执行</li></ul><p>想要了解更多的信息，可以读读<a href=https://www-01.ibm.com/support/knowledgecenter/SSEPGG_9.7.0/com.ibm.db2.luw.admin.perf.doc/doc/c0005311.html target=_blank rel="noopener noreffer">DB2</a>,<a href=https://docs.oracle.com/cd/B28359_01/server.111/b28274/optimops.htm#i76330 target=_blank rel="noopener noreffer">ORACEL</a>或者<a href="https://technet.microsoft.com/en-us/library/ms191426%28v=sql.105%29.aspx" target=_blank rel="noopener noreffer">SQL SERVER</a>的文档。</p><h3 id=444-简化的案例>4.4.4 简化的案例</h3><blockquote><p>We’ve just seen 3 types of join operations.</p><p>Now let’s say we need to join 5 tables to have a full view of a person. A PERSON can have:</p><ul><li>multiple MOBILES</li><li>multiple MAILS</li><li>multiple ADRESSES</li><li>multiple BANK_ACCOUNTS</li></ul></blockquote><p>已经介绍完了三种联结操作。</p><p>现在需要对五个表进行联结表示出一个人的信息。
一个PERSON可以有：</p><ul><li>多个电话</li><li>多个邮箱</li><li>多个地址</li><li>多个银行账户</li></ul><blockquote><p>In other words we need a quick answer for the following query:</p></blockquote><p>换言之，需要写出如下的SQL：</p><pre tabindex=0><code>SELECT * from PERSON, MOBILES, MAILS,ADRESSES, BANK_ACCOUNTS
WHERE
PERSON.PERSON_ID = MOBILES.PERSON_ID
AND PERSON.PERSON_ID = MAILS.PERSON_ID
AND PERSON.PERSON_ID = ADRESSES.PERSON_ID
AND PERSON.PERSON_ID = BANK_ACCOUNTS.PERSON_ID
</code></pre><blockquote><p>As a query optimizer, I have to find the best way to process the data. But there are 2 problems:</p><ul><li>What kind of join should I use for each join?
I have 3 possible joins (Hash Join, Merge Join, Nested Join) with the possibility to use 0,1 or 2 indexes (not to mention that there are different types of indexes).</li><li>What order should I choose to compute the join?</li></ul></blockquote><p>对查询优化器来说，需要找到取数据的最优方式。
但会遇到两个问题：</p><ul><li>对于每个联结来说应该使用什么类型的联结？</li></ul><p>有三种可能的联结方式(哈希联结、合并联结、嵌套联结)，可能使用0、1或2个索引(并不表示有这么多种索引)</p><ul><li>应该按照什么顺序来计算索引？</li></ul><blockquote><p>For example, the following figure shows different possible plans for only 3 joins on 4 tables</p></blockquote><p>举个例子，对于需要进行联结的四张表，有如图所示的联结顺序</p><figure><a class=lightgallery href=/images/join_ordering_problem.png title=/images/join_ordering_problem.png data-thumbnail=/images/join_ordering_problem.png data-sub-html="<h2>联结顺序问题</h2>"><img class=lazyload src=/svg/loading.min.svg data-src=/images/join_ordering_problem.png data-srcset="/images/join_ordering_problem.png, /images/join_ordering_problem.png 1.5x, /images/join_ordering_problem.png 2x" data-sizes=auto alt=/images/join_ordering_problem.png height=1500 width=1500></a><figcaption class=image-caption>联结顺序问题</figcaption></figure><blockquote><p>So here are my possibilities:</p></blockquote><blockquote><ul><li><ol><li>I use a brute force approach
Using the database statistics, I compute the cost for every possible plan and I keep the best one. But there are many possibilities. For a given order of joins, each join has 3 possibilities: HashJoin, MergeJoin, NestedJoin. So, for a given order of joins there ve 3^4 possibilities. The join ordering is a permutation problem on a binary tree and there are (2<em>4)!/(4+1)! possible orders. For this very simplified problem, I end up with 34</em>(2*4)!/(4+1)! possibilities.</li></ol></li></ul></blockquote><blockquote><p>In non-geek terms, it means 27 216 possible plans. If I now add the possibility for the merge join to take 0,1 or 2 B+Tree indexes, the number of possible plans becomes 210 000. Did I forget to mention that this query is VERY SIMPLE?</p></blockquote><p>举几个可能性：</p><ul><li>暴力法
通过使用数据库的统计信息，能计算出每种可能的执行计划，然后用最好的那种。
但是这里有太多种执行方案了。
对于一个给定顺序的联结，每次联结都有三种可能的联结。
因此对于一个给定顺序的联结这里就有$3^4$种可能性。
联结顺序又是一个二叉树排列问题，共有 $(2 * 4)!/(4 + 1)$ 种可能的顺序。
对于相当简单的问题来说，都有 $3^4*(2*4)!/(4+1)$ 种可能性。</li></ul><p>换个通俗的方式来说，意味着有27 216种可能的执行计划。
如果现在对合并联结使用B+树索引的数量给出不同的可能性如1,2,0，执行计划的数量会变成 210 000 种。
还记得我说这还只是一个很简单的查询吗？</p><blockquote><ul><li><ol start=2><li>I cry and quit this job
It’s very tempting but you wouldn’t get your result and I need money to pay the bills.</li></ol></li></ul></blockquote><ul><li>G</li></ul><p>很诱人，但是你查不到结果，我拿不到工资，我们都有美好的未来。</p><blockquote><ul><li><ol start=3><li>I only try a few plans and take the one with the lowest cost.
Since I’m not superman, I can’t compute the cost of every plan. Instead, I can arbitrary choose a subset of all the possible plans, compute their costs and give you the best plan of this subset.</li></ol></li></ul></blockquote><ul><li>仅仅尝试计算几种执行计划并用开销最小的
因为我不是超人，所以不能把每种执行计划的开销都计算出来。
取而代之是从所有<strong>可能的执行计划中随意地选出一个子集</strong>，计算他们的开销并使用开销最小的执行查询。</li></ul><blockquote><ul><li><ol start=4><li>I apply smart rules to reduce the number of possible plans.</li></ol></li></ul><p>There are 2 types of rules:</p><p>I can use “logical” rules that will remove useless possibilities but they won’t filter a lot of possible plans. For example: “the inner relation of the nested loop join must be the smallest data set”</p><p>I accept not finding the best solution and apply more aggressive rules to reduce a lot the number of possibilities. For example “If a relation is small, use a nested loop join and never use a merge join or a hash join”</p></blockquote><ul><li>通过使用一些聪明的规则来<strong>减少可能的执行计划的数量</strong>。</li></ul><p>通常有两种方式：</p><p>可以使用“逻辑的”规则来去除无用的可能性，但这种方法不会过滤掉很多执行计划。
例如：“嵌套循环联结的内联表必须是最小的数据集”</p><p>我接受不能找到开销最小的执行计划的现实，并使用更具侵略性的规则来减少可能的执行计划的数量。
例如“如果一个表很小，只使用嵌套循环联结而不使用另外两种”</p><blockquote><p>In this simple example, I end up with many possibilities. But a real query can have other relational operators like OUTER JOIN, CROSS JOIN, GROUP BY, ORDER BY, PROJECTION, UNION, INTERSECT, DISTINCT … which means even more possibilities.</p></blockquote><blockquote><p>So, how a database does it?</p></blockquote><p>在这个简单的例子中我举了很多种可能性。
但是真正的查询还有其他的表操作例如 OUTER JOIN, CROSS JOIN, GROUP BY, ORDER BY,PROJECTION, UNION, INTERSECT, DISTINCT&mldr; 这意味着更多可能性。</p><p>所以数据库怎么做呢？</p><h3 id=445-动态规划贪心算法和启发式>4.4.5 动态规划、贪心算法和启发式</h3><blockquote><p>A relational database tries the multiple approaches I’ve just said. The real job of an optimizer is to find a good solution on a limited amount of time.</p></blockquote><p>表型数据库尝试过我刚刚说过的几种可能性中的很多种。
优化器的工作是在一定时间内找到一个优秀的执行方案。</p><blockquote><p>Most of the time an optimizer doesn’t find the best solution but a “good” one.</p></blockquote><p><strong>大多数时候优化器并不执着于找到最好，而是找到一个相对“好”的答案</strong>。</p><blockquote><p>For small queries, doing a brute force approach is possible. But there is a way to avoid unnecessary computations so that even medium queries can use the brute force approach. This is called dynamic programming.</p></blockquote><p>对于一些小的查询用暴力法可行。但可以通过一种方法来避免不必要的计算，从而在中等规模的查询上也能使用暴力法。
这种方法叫动态规划。</p><p><strong>动态规划</strong></p><blockquote><p>The idea behind these 2 words is that many executions plan are very similar. If you look at the following plans:</p></blockquote><p>动态规划的主要想法是很多执行计划很类似，如果你看下图：</p><figure><a class=lightgallery href=/images/overlapping_trees.png title=/images/overlapping_trees.png data-thumbnail=/images/overlapping_trees.png data-sub-html="<h2>重叠的树</h2>"><img class=lazyload src=/svg/loading.min.svg data-src=/images/overlapping_trees.png data-srcset="/images/overlapping_trees.png, /images/overlapping_trees.png 1.5x, /images/overlapping_trees.png 2x" data-sizes=auto alt=/images/overlapping_trees.png height=1500 width=1500></a><figcaption class=image-caption>重叠的树</figcaption></figure><blockquote><p>They share the same (A JOIN B) subtree. So, instead of computing the cost of this subtree in every plan, we can compute it once, save the computed cost and reuse it when we see this subtree again. More formally, we’re facing an overlapping problem. To avoid the extra-computation of the partial results we’re using memoization.</p></blockquote><p>他们有相同的子树(A JOIN B)。
因此不必在每次计算的时候都计算一次这颗子树，只算一次，将结果存起来，一旦遇到这个计算的时候直接取结果。
更正式的说法是，重叠子问题。
使用记忆化来避免对部分结果的多余计算。</p><blockquote><p>Using this technique, instead of having a (2*N)!/(N+1)! time complexity, we “just” have 3^N. In our previous example with 4 joins, it means passing from 336 ordering to 81. If you take a bigger query with 8 joins (which is not big), it means passing from 57 657 600 to 6561.</p></blockquote><p>用这种方式，可以吧时间复杂度从$(2*N)!/(N+1)!$降到$3^N$.
在先前的四个联结的例子中，意味着从336降到81.
如果你跑一个更大的有8个联结的查询，能从 57 657 600 降到 6561.</p><blockquote><p>For the CS geeks, here is an algorithm I found on the formal course I already gave you. I won’t explain this algorithm so read it only if you already know dynamic programming or if you’re good with algorithms (you’ve been warned!):</p></blockquote><p>搞个伪代码，懂DP和会算法的看：</p><pre tabindex=0><code>procedure findbestplan(S)
if (bestplan[S].cost infinite)
   return bestplan[S]
// else bestplan[S] has not been computed earlier, compute it now
if (S contains only 1 relation)
         set bestplan[S].plan and bestplan[S].cost based on the best way
         of accessing S  /* Using selections on S and indices on S */
     else for each non-empty subset S1 of S such that S1 != S
   P1= findbestplan(S1)
   P2= findbestplan(S - S1)
   A = best algorithm for joining results of P1 and P2
   cost = P1.cost + P2.cost + cost of A
   if cost &lt; bestplan[S].cost
       bestplan[S].cost = cost
      bestplan[S].plan = “execute P1.plan; execute P2.plan;
                 join results of P1 and P2 using A”
return bestplan[S]
</code></pre><blockquote><p>For bigger queries you can still do a dynamic programming approach but with extra rules (or heuristics) to remove possibilities:</p><ul><li>If we analyze only a certain type of plan (for example: the left-deep trees) we end up with n*2n instead of 3n</li></ul></blockquote><figure><a class=lightgallery href=/images/left-deep-tree.png title=/images/left-deep-tree.png data-thumbnail=/images/left-deep-tree.png data-sub-html="<h2>左深树</h2>"><img class=lazyload src=/svg/loading.min.svg data-src=/images/left-deep-tree.png data-srcset="/images/left-deep-tree.png, /images/left-deep-tree.png 1.5x, /images/left-deep-tree.png 2x" data-sizes=auto alt=/images/left-deep-tree.png height=1500 width=1500></a><figcaption class=image-caption>左深树</figcaption></figure><blockquote><ul><li>If we add logical rules to avoid plans for some patterns (like “if a table as an index for the given predicate, don’t try a merge join on the table but only on the index”) it will reduce the number of possibilities without hurting to much the best possible solution.</li><li>If we add rules on the flow (like “perform the join operations BEFORE all the other relational operations”) it also reduces a lot of possibilities.</li></ul></blockquote><p>对于更大的查询也可以使用动态规划，但是需要加一些限定条件(或者叫<strong>启发</strong>)来减少一些可能性：</p><ul><li>如果我们只分析一种特定烈性的执行计划(如：左深树)，会发现时间复杂度是$n*2^n$而不是$3^n$</li><li>如果加一些逻辑规则来避免在某些模式上使用相应计划(例如“如果一个表的索引以某种形式出现在谓词中，就不去使用基于表的合并联结，只去用索引”)能够减少可能性并且不会太影响到找到最优的执行计划。</li><li>如果我们在流程中添加一些规则(如“在所有其他表操作前执行联结操作”)也能够减少很多可能性</li></ul><p><strong>贪心算法</strong></p><blockquote><p>But for a very big query or to have a very fast answer (but not a very fast query), another type of algorithms is used, the greedy algorithms.</p></blockquote><p>但对于非常大的查询或要有一个非常快的回复(而不是一个非常快的查询)，可以使用贪心算法。</p><blockquote><p>The idea is to follow a rule (or heuristic) to build a query plan in an incremental way. With this rule, a greedy algorithm finds the best solution to a problem one step at a time. The algorithm starts the query plan with one JOIN. Then, at each step, the algorithm adds a new JOIN to the query plan using the same rule.</p></blockquote><p>这个算法遵循某种规则(或<strong>启发</strong>)来以一种增量的方式构建查询计划。
在这个规则下，通过贪心算法寻找在每一步找到当前的最优解。
这个算法从单个JOIN开始。
然后递进的，以同样的规则将新的JOIN加入到查询计划中。</p><blockquote><p>Let’s take a simple example. Let’s say we have a query with 4 joins on 5 tables (A, B, C, D and E). To simplify the problem we just take the nested join as a possible join. Let’s use the rule “use the join with the lowest cost”</p><ul><li>we arbitrary start on one of the 5 tables (let’s choose A)</li><li>we compute the cost of every join with A (A being the inner or outer relation).</li><li>we find that A JOIN B gives the lowest cost.</li><li>we then compute the cost of every join with the result of A JOIN B (A JOIN B being the inner or outer relation).</li><li>we find that (A JOIN B) JOIN C gives the best cost.</li><li>we then compute the cost of every join with the result of the (A JOIN B) JOIN C …</li><li>….</li><li>At the end we find the plan (((A JOIN B) JOIN C) JOIN D) JOIN E)</li></ul></blockquote><p>举一个简单的例子，例如现在需要对五张表进行四次联结。
为了简化问题我们使用嵌套循环联结。
用这个规则“在最低的开销下进行联结”。</p><ul><li>随机从五张表中的一张开始</li><li>对A计算每中联结的开销(A作为内联表或外联表)</li><li>算出A JOIN B的开销最低</li><li>随后计算A JOIN B结果的每种联结的开销</li><li>发现(A JOIN B) JOIN C开销最低</li><li>重复上述步骤</li><li>最后得到执行计划(((A JOIN B) JOIN C) JOIN D) JOIN E</li></ul><blockquote><p>Since we arbitrary started with A, we can apply the same algorithm for B, then C then D then E. We then keep the plan with the lowest cost.</p></blockquote><p>因为是随机选择从A开始的，可以以同样的方式对以B、C、D、E开始的开销都算一遍。
然后从中选择开销最小的。</p><blockquote><p>By the way, this algorithm has a name: it’s called the Nearest neighbor algorithm.</p></blockquote><p>因此这种算法被称作：临近算法。</p><blockquote><p>I won’t go into details, but with a good modeling and a sort in N<em>log(N) this problem can easily be solved. The cost of this algorithm is in O(N</em>log(N)) vs O(3^N) for the full dynamic programming version. If you have a big query with 20 joins, it means 26 vs 3 486 784 401, a BIG difference!</p></blockquote><p>不深入聊细节问题，但是通过良好建模和一个N<em>Log(N)的排序算法这个问题就能轻松解决了。
**这个算法的开销是O(N</em>Log(N))，而完全动态规划的时间复杂度是$O(3^N)$**。
如果一个查询中有20个联结，意味着是 26 比 3 486 784 401，差距显著！</p><blockquote><p>The problem with this algorithm is that we assume that finding the best join between 2 tables will give us the best cost if we keep this join and add a new join. But:</p><ul><li>even if A JOIN B gives the best cost between A, B and C</li><li>(A JOIN C) JOIN B might give a better result than (A JOIN B) JOIN C.</li></ul><p>To improve the result, you can run multiple greedy algorithms using different rules and keep the best plan.</p></blockquote><p>这个算法的问题在于它认为如果我们找到两表中最优的联结方式，通过计算这个联结的结果和新增的表联结的最优解能够得到三表联结的最优解。
但是在这个假设不一定对。</p><p>为何进一步优化结果，可以使用不同的规则多跑几次贪心算法，找最优解。</p><p><strong>其他算法</strong>
[You can skip to the next part, what I’m going to say is not important]</p><blockquote><p>The problem of finding the best possible plan is an active research topic for many CS researchers. They often try to find better solutions for more precise problems/patterns. For example,</p><ul><li>if the query is a star join (it’s a certain type of multiple-join query), some databases will use a specific algorithm.</li><li>if the query is a parallel query, some databases will use a specific algorithm</li></ul></blockquote><p>找到最优的执行计划是一个活跃的计算机研究领域。
计算机科学家们尝试对于不同的问题/模式找到更好的解法，例如：</p><ul><li>如果查询是星型联结(多联结查询中的一种)，某些数据库会使用特定的算法</li><li>如果查询是并行查询，一些数据库会使用特定算法</li></ul><blockquote><p>Other algorithms are also studied to replace dynamic programming for large queries. Greedy algorithms belong to larger family called heuristic algorithms. A greedy algorithm follows a rule (or heuristic), keeps the solution it found at the previous step and “appends” it to find the solution for the current step. Some algorithms follow a rule and apply it in a step-by-step way but don’t always keep the best solution found in the previous step. They are called heuristic algorithms.</p></blockquote><p>也有许多用于在大查询中替换动态规划的算法正在研究中。
贪心算法就是一个叫做启发式算法大类中的一种。
贪心算法遵循某种规则(或启发)，保留其在上一步找到的解，然后“附加”之来找到当前的解。
一些算法也类似这样的模式，遵循某些规则一步步找解，但是并不是每一次前一步的最优解都会保存。
这样的算法都叫启发式算法。</p><blockquote><p>For example, genetic algorithms follow a rule but the best solution of the last step is not often kept:</p><ul><li>A solution represents a possible full query plan</li><li>Instead of one solution (i.e. plan) there are P solutions (i.e. plans) kept at each step.</li><li><ol start=0><li>P query plans are randomly created</li></ol></li><li><ol><li>Only the plans with the best costs are kept</li></ol></li><li><ol start=2><li>These best plans are mixed up to produce P news plans</li></ol></li><li><ol start=3><li>Some of the P new plans are randomly modified</li></ol></li><li><ol start=4><li>The step 1,2,3 are repeated T times</li></ol></li><li><ol start=5><li>Then you keep the best plan from the P plans of the last loop.</li></ol></li></ul></blockquote><p>举个例子，遗传算法遵循某种规则，但最后一步的最优解并不总是保留：</p><ul><li>一个解表示一种可能的查询计划</li><li>遗传算法会在每一步保留很多解，而不是一步只保留一个</li><li><ol start=0><li>P个查询计划随机创建</li></ol></li><li><ol><li>只有开销最低的执行计划保留</li></ol></li><li><ol start=2><li>这些执行计划混合起来生成P个新的执行计划</li></ol></li><li><ol start=3><li>P个计划中的部分计划可能随机修改</li></ol></li><li>重复1、2、3步T次</li><li>然后保留最后一次循环中效果最好的计划</li></ul><blockquote><p>The more loops you do the better the plan will be.</p></blockquote><p>循环的次数越多，得到的执行计划就越好。</p><blockquote><p>Is it magic? No, it’s the laws of nature: only the fittest survives!</p></blockquote><p>这是魔法吗？不是，这是自然法则：适者生存！</p><blockquote><p>FYI, genetic algorithms are implemented in PostgreSQL but I wasn’t able to find if they’re used by default.</p></blockquote><p>仅供参考：说是遗传算法在PostgreSQL实现了，但不知道是不是默认使用的。</p><blockquote><p>There are other heuristic algorithms used in databases like Simulated Annealing, Iterative Improvement, Two-Phase Optimization… But I don’t know if they’re currently used in enterprise databases or if they’re only used in research databases.</p></blockquote><p>还有许多其他用在数据库中的启发算法如模拟退火、迭代提升、两阶段优化等等。
但不知道它们现在是否在商业数据库中用起来了。</p><blockquote><p>For more information, you can read the following research article that presents more possible algorithms: Review of Algorithms for the Join Ordering Problem in Database Query Optimization</p></blockquote><p>可以读<a href=http://www.acad.bg/rismim/itc/sub/archiv/Paper6_1_2009.PDF target=_blank rel="noopener noreffer">这篇文章</a>了解一下相当有可能使用的算法。</p><h3 id=446-真正的优化器unimportant-part>4.4.6 真正的优化器(unimportant part)</h3><p>[You can skip to the next part, what I’m going to say is not important]</p><blockquote><p>But, all this blabla is very theoretical. Since I’m a developer and not a researcher, I like concrete examples.</p></blockquote><p>但刚刚这些讨论都是相当理论的。因为我是开发者而不是研究人员，我喜欢具体的例子。</p><blockquote><p>Let’s see how the SQLite optimizer works. It’s a light database so it uses a simple optimization based on a greedy algorithm with extra-rules to limit the number of possibilities:</p><ul><li>SQLite chooses to never reorder tables in a CROSS JOIN operator</li><li>joins are implemented as nested joins</li><li>outer joins are always evaluated in the order in which they occur</li><li>…</li><li>Prior to version 3.8.0, SQLite uses the “Nearest Neighbor” greedy algorithm when searching for the best query plan</li></ul></blockquote><p>来看看SQLite优化器是怎么工作的。
SQLite是一个轻量级的数据库，使用基于额外规则的贪心算法来减少可能的执行计划数量：</p><ul><li>SQLite在CROSS JOIN操作时永不重排序表</li><li>联结使用嵌套循环联结</li><li>外联结总是按他们出现的顺序进行评估</li><li>&mldr;</li><li>3.8.0中使用了临近贪心算法来找最优解</li></ul><blockquote><p>Wait a minute … we’ve already seen this algorithm! What a coincidence!</p><ul><li>Since version 3.8.0 (released in 2015), SQLite uses the “N Nearest Neighbors” greedy algorithm when searching for the best query plan</li></ul></blockquote><p>刚好讲过这个算法，巧的不得了！</p><ul><li>3.8.0使用的是N临近贪心算法</li></ul><blockquote><p>Let’s see how another optimizer does his job. IBM DB2 is like all the enterprise databases but I’ll focus on this one since it’s the last one I’ve really used before switching to Big Data.</p></blockquote><p>看看其他的优化器怎么运行的。
IBM DB2像所有的商用数据库一样，但因为它是我在切换到Big Data(大数据)前最后一个使用过的数据库，所以我只谈谈它的实现。</p><blockquote><p>If we look at the official documentation, we learn that the DB2 optimizer let you use 7 different levels of optimization:</p><ul><li>Use greedy algorithms for the joins<ul><li>0 – minimal optimization, use index scan and nested-loop join and avoid some Query Rewrite</li><li>1 – low optimization</li><li>2 – full optimization</li></ul></li><li>Use dynamic programming for the joins<ul><li>3 – moderate optimization and rough approximation</li><li>5 – full optimization, uses all techniques with heuristics</li><li>7 – full optimization similar to 5, without heuristics</li><li>9 – maximal optimization spare no effort/expense considers all possible join orders, including Cartesian products</li></ul></li></ul></blockquote><p>如果我们现在去看官方文档，我们能知道DB2优化器可以让你使用7种不同级别的优化：</p><ul><li>使用贪心算法<ul><li>0 - 最小优化，使用索引扫描和嵌套循环联结来避免查询重写</li><li>1 - 低级优化</li><li>2 - 完全优化</li></ul></li><li>使用动态规划<ul><li>3 - 适度优化和粗略近似</li><li>5 - 完全优化，使用所有具有启发式的技术</li><li>7 - 不用启发式的完全优化</li><li>9 - 不遗余力的最大优化，考虑所有可能的联结顺序，包括笛卡尔积产生的</li></ul></li></ul><blockquote><p>We can see that DB2 uses greedy algorithms and dynamic programming. Of course, they don’t share the heuristics they use since the query optimizer is the main power of a database.</p></blockquote><p>可以看到<strong>DB2使用了贪心算法和动态规划</strong>。
这部分并不是开源的，因为数据库优化器是核心技术。</p><blockquote><p>FYI, the default level is 5. By default the optimizer uses the following characteristics:</p><ul><li>All available statistics, including frequent-value and quantile statistics, are used.</li><li>All query rewrite rules (including materialized query table routing) are applied, except computationally intensive rules that are applicable only in very rare cases.</li><li>Dynamic programming join enumeration is used, with:<ul><li>Limited use of composite inner relation</li><li>Limited use of Cartesian products for star schemas involving lookup tables</li></ul></li></ul></blockquote><p>供你参考：默认优化级别是5.优化器默认使用如下特性：</p><ul><li>所有可用的统计数据，包括频率统计和分位数统计</li><li>所有查询重写规则(包括物化查询表路由)都用上了，除了一些计算敏感型的规则仅在很少的情况中使用</li><li>动态规划联结枚举：<ul><li>限制使用复合内部表</li><li>限制涉及星型查找表模式中笛卡尔积的使用</li></ul></li><li>考虑更大范围的访问数据的方法，例如列表预取，索引并联(注：一种特定的对于索引的操作)，物化查询表路由</li></ul><blockquote><p>By default, DB2 uses dynamic programming limited by heuristics for the join ordering.</p></blockquote><p>DB2默认使用由启发式限制的动态规划来找联结顺序。</p><blockquote><p>The others conditions (GROUP BY, DISTINCT…) are handled by simple rules.</p></blockquote><p>其他条件(GROUP BY, DISTINCT)也用相同规则处理.</p><h3 id=447-查询计划缓存>4.4.7 查询计划缓存</h3><blockquote><p>Since the creation of a plan takes time, most databases store the plan into a query plan cache to avoid useless re-computations of the same query plan. It’s kind of a big topic since the database needs to know when to update the outdated plans. The idea is to put a threshold and if the statistics of a table have changed above this threshold then the query plan involving this table is purged from the cache.</p></blockquote><p>因为执行计划的创建需要耗时，大多数数据库将执行计划存在 <strong>查询计划缓存</strong> 中来避免对相同的执行计划进行重复而无用的计算。
这是一个很大的话题，因为数据需要知道何时失效过时的执行计划。
一种实现方式是设置一个阈值，当表的统计数据变化超过阈值的时候，缓存中相应的执行计划将被清除。</p><h2 id=45-查询执行>4.5 查询执行</h2><blockquote><p>At this stage we have an optimized execution plan. This plan is compiled to become an executable code. Then, if there are enough resources (memory, CPU) it is executed by the query executor. The operators in the plan (JOIN, SORT BY …) can be executed in a sequential or parallel way; it’s up to the executor. To get and write its data, the query executor interacts with the data manager, which is the next part of the article.</p></blockquote><p>在这一步已经得到一个优化后的执行计划。
这个执行计划将被编译成可执行代码。
随后如果有相应的资源(内存，CPU)，查询执行器就会执行这个查询计划。
执行计划中的操作可以串行或者并行执行；取决于执行器。
为了对数据进行写操作，查询执行器需要与数据管理器交互，下一章将介绍数据管理器。</p><h1 id=5-数据管理器>5 数据管理器</h1><figure><a class=lightgallery href=/images/data_manager.png title=/images/data_manager.png data-thumbnail=/images/data_manager.png data-sub-html="<h2>数据管理器</h2>"><img class=lazyload src=/svg/loading.min.svg data-src=/images/data_manager.png data-srcset="/images/data_manager.png, /images/data_manager.png 1.5x, /images/data_manager.png 2x" data-sizes=auto alt=/images/data_manager.png height=1500 width=1500></a><figcaption class=image-caption>数据管理器</figcaption></figure><blockquote><p>At this step, the query manager is executing the query and needs the data from the tables and indexes. It asks the data manager to get the data, but there are 2 problems:</p><ul><li>Relational databases use a transactional model. So, you can’t get any data at any time because someone else might be using/modifying the data at the same time.</li><li>Data retrieval is the slowest operation in a database, therefore the data manager needs to be smart enough to get and keep data in memory buffers.</li></ul></blockquote><p>到这一步，查询管理器执行查询并且需要表和索引中的数据。
它向数据管理器要数据，此时会遇到两个问题：</p><ul><li>关系型数据库使用事务的模型。因此你不可能想取就能取到，因为此时可能有其他人正在使用/修改这个数据。</li><li><strong>数据的取回是数据库中最慢的操作</strong>，因此数据管理器需要通过某种聪明的方式将数据放在内存缓冲区。</li></ul><blockquote><p>In this part, we’ll see how relational databases handle these 2 problems. I won’t talk about the way the data manager gets its data because it’s not the most important (and this article is long enough!).</p></blockquote><p>在这一章讲看看关系型数据库是怎么处理这两个问题的。
不会介绍数据管理器是怎么取数据的，因为这部分不那么重要。</p><h2 id=51-缓存管理>5.1 缓存管理</h2><blockquote><p>As I already said, the main bottleneck of databases is disk I/O. To improve performance, modern databases use a cache manager.</p></blockquote><p>如前所述，数据库的主要瓶颈时磁盘IO。
为了提高性能，现代数据库用上了缓存管理器。</p><figure><a class=lightgallery href=/images/cache_manager.png title=/images/cache_manager.png data-thumbnail=/images/cache_manager.png data-sub-html="<h2>数据管理器</h2>"><img class=lazyload src=/svg/loading.min.svg data-src=/images/cache_manager.png data-srcset="/images/cache_manager.png, /images/cache_manager.png 1.5x, /images/cache_manager.png 2x" data-sizes=auto alt=/images/cache_manager.png height=1500 width=1500></a><figcaption class=image-caption>数据管理器</figcaption></figure><blockquote><p>Instead of directly getting the data from the file system, the query executor asks for the data to the cache manager. The cache manager has an in-memory cache called buffer pool. Getting data from memory dramatically speeds up a database. It’s difficult to give an order of magnitude because it depends on the operation you need to do:</p><ul><li>sequential access (ex: full scan) vs random access (ex: access by row id),</li><li>read vs write</li></ul></blockquote><p>与其直接与文件系统打交道，查询执行器向缓存管理器要数据。
缓存管理器有一块内存空间称之为缓冲池。
从内存中取数据能显著提高数据库的性能。
很难给出具体的提升，因为这取决于你的操作类型：</p><ul><li>连续访问(如全表扫描) vs 随机访问(通过行id的访问)</li><li>读 vs 写</li></ul><blockquote><p>and the type of disks used by the database:</p><ul><li>7.2k/10k/15k rpm HDD</li><li>SSD</li><li>RAID 1/5/…{}</li></ul></blockquote><p>并且与数据库使用的磁盘类型有关：</p><ul><li>7.2k/10k/15k rpm HDD</li><li>SSD</li><li>RAID 1/5/…</li></ul><blockquote><p>but I’d say memory is 100 to 100k times faster than disk.</p><p>But, this leads to another problem (as always with databases…). The cache manager needs to get the data in memory BEFORE the query executor uses them; otherwise the query manager has to wait for the data from the slow disks.</p></blockquote><p>但我可以说，内存比磁盘快100到10w倍。</p><p>但是带来了另一个问题。缓存管理器需要在查询执行器使用数据之前将数据移入内存，否则查询管理器就需要等数据从低速磁盘移入内存的过程。</p><h3 id=511-预取>5.1.1 预取</h3><blockquote><p>This problem is called prefetching. A query executor knows the data it’ll need because it knows the full flow of the query and has knowledge of the data on disk with the statistics. Here is the idea:</p><ul><li>When the query executor is processing its first bunch of data</li><li>It asks the cache manager to pre-load the second bunch of data</li><li>When it starts processing the second bunch of data</li><li>It asks the CM to pre-load the third bunch and informs the CM that the first bunch can be purged from cache.</li><li>…</li></ul></blockquote><p>这个问题叫做预取。
查询执行器知道它需要的数据因为它知道查询的全流程并且有磁盘上数据的统计数据。
大体思路如下：</p><ul><li>当查询执行器处理第一批数据时</li><li>它要求缓存管理器提前将第二批数据载入内存</li><li>当它开始处理第二批数据的时候</li><li>它要求缓存管理器将第三批数据载入内存，并且告诉其可以将第一批数据占用的空间回收了</li><li>&mldr;</li></ul><blockquote><p>The CM stores all these data in its buffer pool. In order to know if a data is still needed, the cache manager adds an extra-information about the cached data (called a latch).</p></blockquote><p>缓存管理器将所有的数据都存到它的缓冲池。
为了知道一个数据是否仍需要存在缓冲池，缓存管理器对其缓存的数据加了一个额外标记(叫做<strong>闩</strong>)</p><blockquote><p>Sometimes the query executor doesn’t know what data it’ll need and some databases don’t provide this functionality. Instead, they use a speculative prefetching (for example: if the query executor asked for data 1,3,5 it’ll likely ask for 7,9,11 in a near future) or a sequential prefetching (in this case the CM simply loads from disks the next contiguous data after the ones asked).</p></blockquote><p>有时候查询执行器不知道它需要什么数据，并且有的数据库不提供这样的功能。
取而代之的是，它们使用的是一种推测性的预取(例如：如果查询执行器要1,3,5，它有可能需要在之后取7,9,11)或者顺序预取(例如缓存管理器在取完一块数据后把其后的一块数据也读进来。)</p><blockquote><p>To monitor how well the prefetching is working, modern databases provide a metric called buffer/cache hit ratio. The hit ratio shows how often a requested data has been found in the buffer cache without requiring disk access.</p></blockquote><p>为了观测预取的效果，现代数据提供了一种叫做缓存命中率的指标。
命中率展示了访问数据可直接从缓冲池中读出而无需访问磁盘的频率。</p><blockquote><p>Note: a poor cache hit ratio doesn’t always mean that the cache is ill-working. For more information, you can read the Oracle documentation.</p></blockquote><p>注：命中率低并不意味着缓存工作不佳，感兴趣的可以看看<a href=https://docs.oracle.com/database/121/TGDBA/tune_buffer_cache.htm target=_blank rel="noopener noreffer">这篇文章</a></p><blockquote><p>But, a buffer is a limited amount of memory. Therefore, it needs to remove some data to be able to load new ones. Loading and purging the cache has a cost in terms of disk and network I/O. If you have a query that is often executed, it wouldn’t be efficient to always load then purge the data used by this query. To handle this problem, modern databases use a buffer replacement strategy.</p></blockquote><p>但是缓存的内存空间是有限的。因此它需要将一些数据移出才能载入新数据。
载入和清除缓存是有磁盘和网络IO开销的。
如果有一个经常执行的查询，每次执行都对其需要的数据先载入再清除效率很低。
为了解决这个问题，现代数据库使用缓存替换策略。</p><h3 id=512-缓存替换策略>5.1.2 缓存替换策略</h3><blockquote><p>Most modern databases (at least SQL Server, MySQL, Oracle and DB2) use an LRU algorithm.</p></blockquote><p>现代数据库(至少有SQL Server，MySQL，Oracle和DB2)使用LRU算法。</p><blockquote><p>LRU stands for Least Recently Used. The idea behind this algorithm is to keep in the cache the data that have been recently used and, therefore, are more likely to be used again.</p></blockquote><p>LRU是最近最常使用的缩写。
算法的核心思想是那些最近使用过的数据很有可能再次使用。</p><figure><a class=lightgallery href=/images/LRU.png title=/images/LRU.png data-thumbnail=/images/LRU.png data-sub-html="<h2>LRU</h2>"><img class=lazyload src=/svg/loading.min.svg data-src=/images/LRU.png data-srcset="/images/LRU.png, /images/LRU.png 1.5x, /images/LRU.png 2x" data-sizes=auto alt=/images/LRU.png height=1500 width=1500></a><figcaption class=image-caption>LRU</figcaption></figure><blockquote><p>For the sake of comprehension, I’ll assume that the data in the buffer are not locked by latches (and therefore can be removed). In this simple example the buffer can store 3 elements:</p><ul><li>1: the cache manager uses the data 1 and puts the data into the empty buffer</li><li>2: the CM uses the data 4 and puts the data into the half-loaded buffer</li><li>3: the CM uses the data 3 and puts the data into the half-loaded buffer</li><li>4: the CM uses the data 9. The buffer is full so data 1 is removed since it’s the last recently used data. Data 9 is added into the buffer</li><li>5: the CM uses the data 4. Data 4 is already in the buffer therefore it becomes the first recently used data again.</li><li>6: the CM uses the data 1. The buffer is full so data 9 is removed since it’s the last recently used data. Data 1 is added into the buffer</li><li>…</li></ul></blockquote><p>为了方便理解，假设缓冲区的数据都是没有上闩的(因此可以被移除)。
在这个示例中，缓冲区可以存三个元素：</p><ul><li>1: 缓存管理器(CM)用到1，然后将其放入空的缓冲区</li><li>2: CM用到4，并放入缓冲区</li><li>3: CM用到3，放入缓冲区，此时</li><li>4: CM用到9，缓冲区已满，因为1是最久没有使用过的数据，因此移出，将9放入缓冲区</li><li>5: CM用到4，因为已经在缓冲区，将其移动到头部</li><li>6: CM用到1，缓冲区已满，因为3是最久没有使用过的数据，因此移出</li></ul><blockquote><p>This algorithm works well but there are some limitations. What if there is a full scan on a large table? In other words, what happens when the size of the table/index is above the size of the buffer? Using this algorithm will remove all the previous values in the cache whereas the data from the full scan are likely to be used only once.</p></blockquote><p>这个算法用起来很不错，但有些限制。
如果此时有一个对大表的全表扫描呢？
换言之，如果一次对表/索引查询的数据范围大于缓冲区的大小呢？
如果在这种场景依然使用这个算法，缓冲区原本的数据将全部移出被全表扫描的数据替代，而这些数据可能只会用一次。</p><p><strong>改进</strong></p><blockquote><p>To prevent this to happen, some databases add specific rules. For example according to Oracle documentation:
“For very large tables, the database typically uses a direct path read, which loads blocks directly […], to avoid populating the buffer cache. For medium size tables, the database may use a direct read or a cache read. If it decides to use a cache read, then the database places the blocks at the end of the LRU list to prevent the scan from effectively cleaning out the buffer cache.”</p></blockquote><p>为了避免这种现象发生，一些数据库加了特定的规则。
如Oracle文档所述：</p><p>对于非常大的表，数据库通常使用直接将块载入的访问方式，来避免污染缓冲区。对于中型表，数据库可能直接访问也可能载入缓存。如果决定载入缓冲区，会将这个块放在LRU列表的尾部，以免扫描把缓冲区洗得太彻底。</p><blockquote><p>There are other possibilities like using an advanced version of LRU called LRU-K. For example SQL Server uses LRU-K for K =2.</p></blockquote><p>同样还有可能使用一些LRU的进阶版本例如LRU-K。
例如SQL Server使用LRU-2.</p><blockquote><p>This idea behind this algorithm is to take into account more history. With the simple LRU (which is also LRU-K for K=1), the algorithm only takes into account the last time the data was used. With the LRU-K:</p><ul><li>It takes into account the K last times the data was used.</li><li>A weight is put on the number of times the data was used</li><li>If a bunch of new data is loaded into the cache, the old but often used data are not removed (because their weights are higher).</li><li>But the algorithm can’t keep old data in the cache if they aren’t used anymore.</li><li>So the weights decrease over time if the data is not used.</li></ul></blockquote><p>这个算法的大体思路是考虑更多历史访问记录。
在LRU-1中，算法只考虑最后一次数据使用的情况。
在LRU-K中：</p><ul><li>考虑数据最后被使用的K次</li><li>对数据使用次数赋一个权重</li><li>如果一批数据载入缓冲，旧的但使用频繁的数据不会被移出缓冲区(因为它们权重高)</li><li>但是算法也不能将那些不怎么使用的旧数据一直放在缓冲区</li><li>因此不使用的数据的权重会随时间下降</li></ul><blockquote><p>The computation of the weight is costly and this is why SQL Server only uses K=2. This value performs well for an acceptable overhead.</p></blockquote><p>权重的计算开销很大，因此SQL Server只使用了LRU-2.
这个值表现得不错，并且性能损耗不明显。</p><blockquote><p>For a more in-depth knowledge of LRU-K, you can read the original research paper (1993): The LRU-K page replacement algorithm for database disk buffering.</p></blockquote><p>如果想了解更多内容可以读下<a href=https://www.cs.cmu.edu/~christos/courses/721-resources/p297-o_neil.pdf target=_blank rel="noopener noreffer">这篇文章</a></p><blockquote><p>Other algorithms
Of course there are other algorithms to manage cache like</p><ul><li>2Q (a LRU-K like algorithm)</li><li>CLOCK (a LRU-K like algorithm)</li><li>MRU (most recently used, uses the same logic than LRU but with another rule)</li><li>LRFU (Least Recently and Frequently Used)</li><li>…</li></ul><p>Some databases let the possibility to use another algorithm than the default one.</p></blockquote><p><strong>其他算法</strong></p><p>还有其他的管理缓存的算法例如：</p><ul><li>2Q(一种类似LRU-K的算法)</li><li>CLOCK(一种类似LRU-K的算法)</li><li>MRU(最常用，和LRU逻辑相同，但使用另一种规则)</li><li>LRFU(最近最常且最频繁使用)</li></ul><p>一些数据库可以选择管理缓冲区的算法。</p><h3 id=513-写缓存>5.1.3 写缓存</h3><blockquote><p>I only talked about read buffers that load data before using them. But in a database you also have write buffers that store data and flush them on disk by bunches instead of writing data one by one and producing many single disk accesses.</p></blockquote><p>这里只讲到读缓存需要在使用前将数据载入。
但在数据库中你同样需要写缓存来暂存数据并在随后刷入磁盘，而不是每次写入都落盘。</p><blockquote><p>Keep in mind that buffers store pages (the smallest unit of data) and not rows (which is a logical/human way to see data). A page in a buffer pool is dirty if the page has been modified and not written on disk. There are multiple algorithms to decide the best time to write the dirty pages on disk but it’s highly linked to the notion of transaction, which is the next part of the article.</p></blockquote><p>需要记住的是缓存区存的是页而不是行。
如果缓存区中的页被修改且并没有刷入磁盘，就称这个页是<strong>脏页</strong>。
有很多种决定什么时候将脏页落盘的算法，但是这与接下来要讨论的事务的概念密切相关。</p><h2 id=52-事务管理>5.2 事务管理</h2><blockquote><p>Last but not least, this part is about the transaction manager. We’ll see how this process ensures that each query is executed in its own transaction. But before that, we need to understand the concept of ACID transactions.</p></blockquote><p>最后但同样重要的一章是事务管理器。
我们将看到进程如何确保每个查询都在自己的事务中执行。
但在那之前，需要了解一下事务中ACID的概念。</p><h3 id=521-酸了>5.2.1 酸了</h3><blockquote><p>An ACID transaction is a unit of work that ensures 4 things:</p><ul><li>Atomicity: the transaction is “all or nothing”, even if it lasts 10 hours. If the transaction crashes, the state goes back to before the transaction (the transaction is rolled back).</li><li>Isolation: if 2 transactions A and B run at the same time, the result of transactions A and B must be the same whether A finishes before/after/during transaction B.</li><li>Durability: once the transaction is committed (i.e. ends successfully), the data stay in the database no matter what happens (crash or error).</li><li>Consistency: only valid data (in terms of relational constraints and functional constraints) are written to the database. The consistency is related to atomicity and isolation.</li></ul></blockquote><p>一个ACID的事务指的是一组满足如下四件事的工作：</p><ul><li><strong>原子性</strong>：事务指的是“完成或者不做”，即使持续10h。如果一个事务崩溃，那么系统的状态应该恢复到事务开始前(称这个事务被<strong>回滚</strong>了)。</li><li><strong>隔离性</strong>：如果两个事务A、B同时启动，最后执行的结果应该与A在B之前/之后/之中完成一样。</li><li><strong>持久性</strong>：一旦一个事务被<strong>提交</strong>了，那么数据无论发生什么都应该存在数据库中。</li><li><strong>一致性</strong>：只有合法的数据(符合关系约束或者功能约束)能写入数据库。一致性与原子、隔离性相关。</li></ul><blockquote><p>During the same transaction, you can run multiple SQL queries to read, create, update and delete data. The mess begins when two transactions are using the same data. The classic example is a money transfer from an account A to an account B. Imagine you have 2 transactions:</p><ul><li>Transaction 1 that takes 100$ from account A and gives them to account B</li><li>Transaction 2 that takes 50$ from account A and gives them to account B</li></ul></blockquote><p>在同一个事务中可以跑多条进行读、创建、更新、删除的SQL。
蛋疼的事情发生在两个事务使用同一个数据的时候。
经典的例子是转账问题。想象这样两个事务：</p><ul><li>T1将100从A账户转到B账户</li><li>T2将50从A账户转到B账户</li></ul><blockquote><p>If we go back to the ACID properties:</p><ul><li>Atomicity ensures that no matter what happens during T1 (a server crash, a network failure …), you can’t end up in a situation where the 100$ are withdrawn from A and not given to B (this case is an inconsistent state).</li><li>Isolation ensures that if T1 and T2 happen at the same time, in the end A will be taken 150$ and B given 150$ and not, for example, A taken 150$ and B given just $50 because T2 has partially erased the actions of T1 (this case is also an inconsistent state).</li><li>Durability ensures that T1 won’t disappear into thin air if the database crashes just after T1 is committed.</li><li>Consistency ensures that no money is created or destroyed in the system.</li></ul></blockquote><p>此时回到ACID：</p><ul><li>原子性确保不论在T1中发生什么(服务器崩溃，网络故障)，不能发生A扣钱了，但是B没收到钱的事情(不一致的状态)</li><li>隔离性保证及时T1和T2同时发生，结果是A转了150给B，而不是A扣了150，而B只收到50，因为T2部分抹去T1的操作</li><li>持久性确保T1在提交后不会在数据库崩溃后灰飞烟灭</li><li>一致性确保在系统中没有钱凭空创造或者消失</li></ul><p><em>[You can skip to the next part if you want, what I’m going to say is not important for the rest of the article]</em></p><blockquote><p>Many modern databases don’t use a pure isolation as a default behavior because it comes with a huge performance overhead. The SQL norm defines 4 levels of isolation:</p><ul><li>Serializable (default behaviour in SQLite): The highest level of isolation. Two transactions happening at the same time are 100% isolated. Each transaction has its own “world”.</li><li>Repeatable read (default behavior in MySQL): Each transaction has its own “world” except in one situation. If a transaction ends up successfully and adds new data, these data will be visible in the other and still running transactions. But if A modifies a data and ends up successfully, the modification won’t be visible in the still running transactions. So, this break of isolation between transactions is only about new data, not the existing ones.</li></ul><p>For example, if a transaction A does a “SELECT count(1) from TABLE_X” and then a new data is added and committed in TABLE_X by Transaction B, if transaction A does again a count(1) the value won’t be the same.</p><p>This is called a phantom read.</p><ul><li>Read committed (default behavior in Oracle, PostgreSQL and SQL Server): It’s a repeatable read + a new break of isolation. If a transaction A reads a data D and then this data is modified (or deleted) and committed by a transaction B, if A reads data D again it will see the modification (or deletion) made by B on the data.</li></ul><p>This is called a non-repeatable read.</p><ul><li>Read uncommitted: the lowest level of isolation. It’s a read committed + a new break of isolation. If a transaction A reads a data D and then this data D is modified by a transaction B (that is not committed and still running), if A reads data D again it will see the modified value. If transaction B is rolled back, then data D read by A the second time doesn’t make no sense since it has been modified by a transaction B that never happened (since it was rolled back).</li></ul><p>This is called a dirty read.</p></blockquote><p>很多现代数据库并不适用纯粹的隔离最为默认级别，因为会带来巨大的性能损耗。
SQL定义了四种隔离级别：</p><ul><li>串行化(SQLite默认隔离级别)：最高的隔离级别。同时进行的两个事务100%隔离。每个事物都有自己的小世界。</li><li>可重复读(MySQL默认隔离级别)：每个事物都有自己的小世界，除了一种情况。如果一个事务最后成功结束并新增了数据，这些数据会被其他正在执行的事务读到。但对数据的改动不会被看到。所以打破隔离的只有新增数据。</li></ul><p>例如如果TA执行<code>SELECT count(1) from TABLE_X</code>并且随后TB对TABLE+X插入数据，TA再执行一次SQL得到的结果将不一样。</p><p>这个现象叫做<strong>幻读</strong>。</p><ul><li>读提交(Oracle,PostgreSQL,SQL Server默认隔离级别)：不可重复读。如果TA读数据D，并且随后这个数据被改动了，改动被TB提交了，A再次读D将看到这个改动。</li></ul><p>这个现象叫做<strong>不可重复读</strong>。</p><ul><li>读未提交：最低的隔离级别。如果事务A读数据D，随后数据D被事务B修改，A将读到D的改动。随后如果事务B回滚，A对数据D的第二次读将毫无意义，因为被回滚了。</li></ul><p>这个现象叫做<strong>脏读</strong>。</p><blockquote><p>Most databases add their own custom levels of isolation (like the snapshot isolation used by PostgreSQL, Oracle and SQL Server). Moreover, most databases don’t implement all the levels of the SQL norm (especially the read uncommitted level).</p></blockquote><p>大多数数据库会有自己实现的隔离级别(例如快照隔离级别)。
除此之外，大多数数据库并没有实现所有SQL规范规定的隔离级别，例如读未提交。</p><blockquote><p>The default level of isolation can be overridden by the user/developer at the beginning of the connection (it’s a very simple line of code to add).</p></blockquote><p>默认隔离级别可以在用户/开发者在连接初始化的时候覆盖。</p><h3 id=522-并发控制>5.2.2 并发控制</h3><blockquote><p>The real issue to ensure isolation, coherency and atomicity is the write operations on the same data (add, update and delete):</p><ul><li>if all transactions are only reading data, they can work at the same time without modifying the behavior of another transaction.</li><li>if (at least) one of the transactions is modifying a data read by other transactions, the database needs to find a way to hide this modification from the other transactions. Moreover, it also needs to ensure that this modification won’t be erased by another transaction that didn’t see the modified data.</li></ul></blockquote><p>真正的问题在于确保 <strong>对同样的数据进行写操作时</strong> 的隔离、一致、原子性：</p><ul><li>如果所有的事务都是读操作，那么它们可以同时工作而无需修改其他事务</li><li>如果至少有一个事务修改了其他事务正在读的数据，数据库需要找到一种方式来对其他事务隐藏这个修改。除此之外，还需要确保其他未看到这个修改的事务不会将这次修改抹除。</li></ul><blockquote><p>This problem is a called concurrency control.</p></blockquote><p>这个问题叫做 <strong>并发控制</strong> 。</p><blockquote><p>The easiest way to solve this problem is to run each transaction one by one (i.e. sequentially). But that’s not scalable at all and only one core is working on the multi-processor/core server, not very efficient…</p></blockquote><p>处理这个问题最简单的方式就是逐事务进行(例如可串行化的隔离级别)。
但是这种方式根本无法扩展，并且在多核/线程的服务器上效率很低。</p><blockquote><p>The ideal way to solve this problem is, every time a transaction is created or cancelled:</p><ul><li>to monitor all the operations of all the transactions</li><li>to check if the parts of 2 (or more) transactions are in conflict because they’re reading/modifying the same data.</li><li>to reorder the operations inside the conflicting transactions to reduce the size of the conflicting parts</li><li>to execute the conflicting parts in a certain order (while the non-conflicting transactions are still running concurrently).</li><li>to take into account that a transaction can be cancelled.</li></ul></blockquote><p>对这个问题最理想的解决方式是，当有新事务创建或销毁时：</p><ul><li>监控所有事务的所有操作</li><li>检查是否有对同一数据的不同事务的操作有读写冲突</li><li>对那些有冲突的事务进行操作重排序来减少冲突部分</li><li>对冲突部分按照特定的顺序执行(无冲突的部分并发执行)</li><li>需要考虑事务是可能被取消的</li></ul><blockquote><p>More formally it’s a scheduling problem with conflicting schedules. More concretely, it’s a very difficult and CPU-expensive optimization problem. Enterprise databases can’t afford to wait hours to find the best schedule for each new transaction event. Therefore, they use less ideal approaches that lead to more time wasted between conflicting transactions.</p></blockquote><p>更正式的说法是有冲突的调度问题。
更具体地说，这是一件非常难且CPU开销很大的优化任务。
商用数据库无法承受为了找到最好的调度计划对每个新事务等待几个小时。
因此，他们使用一种不太理想的方法，导致在相互冲突的事务之间花费更多时间。</p><h3 id=523-锁管理>5.2.3 锁管理</h3><blockquote><p>To handle this problem, most databases are using locks and/or data versioning. Since it’s a big topic, I’ll focus on the locking part then I’ll speak a little bit about data versioning.</p></blockquote><p>为了解决这个问题，许多数据库使用 <strong>锁</strong> 且/或 <strong>数据版本</strong> 。
因为这是一个很大的话题，我会聚焦在锁部分，然后简单提一嘴数据版本。</p><p><strong>悲观锁</strong></p><blockquote><p>The idea behind locking is:</p><ul><li>if a transaction needs a data,</li><li>it locks the data</li><li>if another transaction also needs this data,</li><li>it’ll have to wait until the first transaction releases the data.</li></ul></blockquote><p>锁背后的思路是：</p><ul><li>如果一个事务需要数据</li><li>它将相应数据上锁</li><li>如果其他事务需要这个数据</li><li>将会等待直到第一个事务释放锁</li></ul><blockquote><p>This is called an exclusive lock.</p></blockquote><p>这叫做 <strong>互斥锁</strong> 。</p><blockquote><p>But using an exclusive lock for a transaction that only needs to read a data is very expensive since it forces other transactions that only want to read the same data to wait. This is why there is another type of lock, the shared lock.</p></blockquote><p>但对于只读事务来说使用互斥锁非常昂贵，因为它 <strong>强迫其他想要读这个数据的事务等待</strong>。
这就是为什么还有一种叫做 <strong>共享锁</strong> 的锁。</p><blockquote><p>With the shared lock:</p><ul><li>if a transaction needs only to read a data A,</li><li>it “shared locks” the data and reads the data</li><li>if a second transaction also needs only to read data A,</li><li>it “shared locks” the data and reads the data</li><li>if a third transaction needs to modify data A,</li><li>it “exclusive locks” the data but it has to wait until the 2 other transactions release their shared locks to apply its exclusive lock on data A.</li></ul></blockquote><p>通过共享锁：</p><ul><li>如果一个事务需要读A</li><li>它给数据上共享锁并读取</li><li>如果另一个数据也想读A</li><li>它也对数据加共享锁并读取</li><li>第三个事务想修改A</li><li>它尝试给A加上互斥锁，但是需要等另外两个事务释放他们的共享锁，随后才能给其加上互斥锁</li></ul><blockquote><p>Still, if a data as an exclusive lock, a transaction that just needs to read the data will have to wait the end of the exclusive lock to put a shared lock on the data.</p></blockquote><p>与此同时，如果一个数据已经被上了互斥锁，另一个事务想要读这个数据也需要等到互斥锁被释放，加上共享锁以后才能读取。</p><figure><a class=lightgallery href=/images/lock_manager.png title=/images/lock_manager.png data-thumbnail=/images/lock_manager.png data-sub-html="<h2>锁管理</h2>"><img class=lazyload src=/svg/loading.min.svg data-src=/images/lock_manager.png data-srcset="/images/lock_manager.png, /images/lock_manager.png 1.5x, /images/lock_manager.png 2x" data-sizes=auto alt=/images/lock_manager.png height=1500 width=1500></a><figcaption class=image-caption>锁管理</figcaption></figure><blockquote><p>The lock manager is the process that gives and releases locks. Internally, it stores the locks in a hash table (where the key is the data to lock) and knows for each data:</p><ul><li>which transactions are locking the data</li><li>which transactions are waiting for the data</li></ul></blockquote><p>锁管理器是对锁进行分配和释放的进程。
从内部看，它将锁信息存储在哈希表中(date->lock)并且知道每个数据的如下信息：</p><ul><li>哪个事务锁住了数据</li><li>哪个事务在等待对这个数据加锁</li></ul><p><strong>死锁</strong></p><blockquote><p>But the use of locks can lead to a situation where 2 transactions are waiting forever for a data:</p></blockquote><figure><a class=lightgallery href=/images/deadlock.png title=/images/deadlock.png data-thumbnail=/images/deadlock.png data-sub-html="<h2>死锁</h2>"><img class=lazyload src=/svg/loading.min.svg data-src=/images/deadlock.png data-srcset="/images/deadlock.png, /images/deadlock.png 1.5x, /images/deadlock.png 2x" data-sizes=auto alt=/images/deadlock.png height=1500 width=1500></a><figcaption class=image-caption>死锁</figcaption></figure><p>但是使用锁可能导致两个事务一直在相互等待。</p><blockquote><p>In this figure:</p><ul><li>transaction A has an exclusive lock on data1 and is waiting to get data2</li><li>transaction B has an exclusive lock on data2 and is waiting to get data1</li></ul></blockquote><p>在图中：</p><ul><li>事务A有数据1的互斥锁，并等待给数据2上锁</li><li>事务B有数据2的互斥锁，并等待给数据1上锁</li></ul><blockquote><p>This is called a deadlock.</p><p>During a deadlock, the lock manager chooses which transaction to cancel (rollback) in order to remove the deadlock. This decision is not easy:</p><ul><li>Is it better to kill the transaction that modified the least amount of data (and therefore that will produce the least expensive rollback)?</li><li>Is it better to kill the least aged transaction because the user of the other transaction has waited longer?</li><li>Is it better to kill the transaction that will take less time to finish (and avoid a possible starvation)?</li><li>In case of rollback, how many transactions will be impacted by this rollback?</li></ul></blockquote><p>这种现象叫做 <strong>死锁</strong>。</p><p>在死锁中，锁管理器为了解除死锁需要选择事务进行回滚。
决定对谁回滚并不容易：</p><ul><li>回滚那些修改量很小的事务更好吗(这样回滚的代价会小一些)？</li><li>因为执行时间更长的事务的用户等待地更久，所以回滚那些执行时间最少的事务更好吗？</li><li>回滚那些耗时更短的事务更好吗(这样能避免饥饿)？</li><li>当涉及回滚的时候，多少事务会被这个回滚的事务影响？</li></ul><blockquote><p>But before making this choice, it needs to check if there are deadlocks.</p></blockquote><p>回滚前需要进行死锁检查。</p><blockquote><p>The hash table can be seen as a graph (like in the previous figures). There is a deadlock if there is a cycle in the graph. Since it’s expensive to check for cycles (because the graph with all the locks is quite big), a simpler approach is often used: using a timeout. If a lock is not given within this timeout, the transaction enters a deadlock state.</p></blockquote><p>哈希表可以看作图(像前面的图所示)。
图中有圈表明有死锁。
因为检查是否有圈是一个很昂贵的操作(存着所有锁信息的图通常很大)，一个简单的经常被使用的方式是：使用超时。
如果锁没有在这个超时时间内分配，表明这个事务陷入死锁了。</p><blockquote><p>The lock manager can also check before giving a lock if this lock will create a deadlock. But again it’s computationally expensive to do it perfectly. Therefore, these pre-checks are often a set of basic rules.</p></blockquote><p>锁管理器同样需要在分配锁之前检查是否会造成死锁。
因为检查图中是否有圈是计算复杂型任务，所以预检查通常是一组基本规则。</p><p><strong>两阶段锁</strong></p><blockquote><p>The simplest way to ensure a pure isolation is if a lock is acquired at the beginning of the transaction and released at the end of the transaction. This means that a transaction has to wait for all its locks before it starts and the locks held by a transaction are released when the transaction ends. It works but it produces a lot of time wasted to wait for all locks.</p></blockquote><p>确保纯粹隔离级别 <strong>最简单的方式</strong> 是在事务开始的时候就申请锁，在事务结束后释放锁。
这意味着事务在开始前需要等到所有的锁，并且在整个事务持续过程中都持有这些锁知道事务结束才释放。
这种方式行得通但是在等待锁期间会 <strong>带来巨额时间开销</strong>。</p><blockquote><p>A faster way is the Two-Phase Locking Protocol (used by DB2 and SQL Server) where a transaction is divided into 2 phases:</p><ul><li>the growing phase where a transaction can obtain locks, but can’t release any lock.</li><li>the shrinking phase where a transaction can release locks (on the data it has already processed and won’t process again), but can’t obtain new locks.</li></ul></blockquote><p>一种更快的方式叫 <strong>两阶段锁协议</strong>(DB2和SQL Server在用)，将事务分为两个阶段：</p><ul><li><strong>增长阶段</strong>：事务可以获取锁，但是不能释放锁</li><li><strong>收缩阶段</strong>：事务可以释放锁，但无法获取锁</li></ul><figure><a class=lightgallery href=/images/two-phase-locking.png title=/images/two-phase-locking.png data-thumbnail=/images/two-phase-locking.png data-sub-html="<h2>两阶段锁协议</h2>"><img class=lazyload src=/svg/loading.min.svg data-src=/images/two-phase-locking.png data-srcset="/images/two-phase-locking.png, /images/two-phase-locking.png 1.5x, /images/two-phase-locking.png 2x" data-sizes=auto alt=/images/two-phase-locking.png height=1500 width=1500></a><figcaption class=image-caption>两阶段锁协议</figcaption></figure><blockquote><p>The idea behind these 2 simple rules is:</p><ul><li>to release the locks that aren’t used anymore to reduce the wait time of other transactions waiting for these locks</li><li>to prevent from cases where a transaction gets data modified after the transaction started and therefore aren’t coherent with the first data the transaction acquired.</li></ul></blockquote><p>这个想法背后是这样两个简单的规则：</p><ul><li>及时释放不再需要使用数据上的锁以减少其他事务的等待时间</li><li>以防在事务开始后获得的数据被修改，从而与事务第一个访问到的数据不一致的情况</li></ul><blockquote><p>This protocol works well except if a transaction that modified a data and released the associated lock is cancelled (rolled back). You could end up in a case where another transaction reads the modified value whereas this value is going to be rolled back. To avoid this problem, all the exclusive locks must be released at the end of the transaction.</p></blockquote><p>这个协议在没有涉及到回滚的时候运行得很好。
在回滚时，可能遇到另一个事务读被修改的值，此时这个值将被回滚。
为了避免这种情况，<strong>所有的互斥锁都必须在事务的最后释放</strong>。</p><p><strong>再说一点</strong></p><blockquote><p>Of course a real database uses a more sophisticated system involving more types of locks (like intention locks) and more granularities (locks on a row, on a page, on a partition, on a table, on a tablespace) but the idea remains the same.</p></blockquote><p>真正的数据库当然使用更加精密设计的系统，有更多的锁类型(像意向锁)和更多粒度(行级锁，页级锁，分区锁，表级锁，表空间级锁)但是思路一致。</p><blockquote><p>I only presented the pure lock-based approach. Data versioning is another way to deal with this problem.</p></blockquote><p>我只介绍了纯粹的基于锁的方式。
<strong>数据版本是另一种处理并发控制的方式。</strong></p><blockquote><p>The idea behind versioning is that:</p><ul><li>every transaction can modify the same data at the same time</li><li>each transaction has its own copy (or version) of the data</li><li>if 2 transactions modify the same data, only one modification will be accepted, the other will be refused and the associated transaction will be rolled back (and maybe re-run).</li></ul></blockquote><p>多版本的思路是：</p><ul><li>不同的事务可以同时对相同数据进行修改</li><li>不同事务有其对这个数据的单独副本</li><li>如果两个事务改了相同的数据，只有一个修改会被接受，另一个会被拒绝，并且相应的事务会被回滚。</li></ul><blockquote><p>It increases the performance since:</p><ul><li>reader transactions don’t block writer transactions</li><li>writer transactions don’t block reader transactions</li><li>there is no overhead from the “fat and slow” lock manager</li></ul></blockquote><p>它能够提升性能表现：</p><ul><li><strong>读事务不会阻塞写事务</strong></li><li><strong>写事务不会阻塞读事务</strong></li><li>没有来自臃肿的锁管理器的开销</li></ul><blockquote><p>Everything is better than locks except when 2 transactions write the same data. Moreover, you can quickly end up with a huge disk space overhead.</p></blockquote><p>当两个事务没有对同一个数据进行写操作的时候性能都比基于锁的并发控制要好。
此外，你可能很快遇到巨额磁盘空间开销。</p><blockquote><p>Data versioning and locking are two different visions: optimistic locking vs pessimistic locking. They both have pros and cons; it really depends on the use case (more reads vs more writes). For a presentation on data versioning, I recommend this very good presentation on how PostgreSQL implements multiversion concurrency control.</p></blockquote><p>数据版本和锁是两种不同的方式：<strong>乐观锁vs悲观锁</strong>。
他们都有各自的优缺点；需要取决于使用的场景。
对于数据版本的pre我推荐<a href=http://momjian.us/main/writings/pgsql/mvcc.pdf target=_blank rel="noopener noreffer">这个</a>介绍PostgreSQL如何实现多版本并发控制的。</p><blockquote><p>Some databases like DB2 (until DB2 9.7) and SQL Server (except for snapshot isolation) are only using locks. Other like PostgreSQL, MySQL and Oracle use a mixed approach involving locks and data versioning. I’m not aware of a database using only data versioning (if you know a database based on a pure data versioning, feel free to tell me).</p></blockquote><p>一些数据库仅使用锁来实现并发控制，例如DB2和SQL Server。
其他数据库使用混合两种方式实现并发控制，例如PostgreSQL,MySQL,Oracle.
我不知道有没有数据库只用多数据版本来实现并发控制。</p><p>注：原文补充了两个只用版本实现并发控制的数据库。</p><blockquote><p>If you read the part on the different levels of isolation, when you increase the isolation level you increase the number of locks and therefore the time wasted by transactions to wait for their locks. This is why most databases don’t use the highest isolation level (Serializable) by default.</p></blockquote><p>在你读到不同隔离级别的时候你会发现，当你增加隔离级别时，锁的数量也会增加，因此事务等待锁的时间也会增加。
这也是为什么大多数数据库并不使用最高的隔离级别(可串行化)。</p><blockquote><p>As always, you can check by yourself in the documentation of the main databases (for example MySQL, PostgreSQL or Oracle).</p></blockquote><p>你可以自己看看主流数据库的文档(例如<a href=https://dev.mysql.com/doc/refman/5.7/en/innodb-transaction-model.html target=_blank rel="noopener noreffer">MySQL</a>,<a href=https://www.postgresql.org/docs/9.4/static/mvcc.html target=_blank rel="noopener noreffer">PostgreSQL</a>,<a href=https://docs.oracle.com/cd/B28359_01/server.111/b28318/consist.htm#i5337 target=_blank rel="noopener noreffer">Oracle</a>)</p><h3 id=524-日志管理>5.2.4 日志管理</h3><blockquote><p>We’ve already seen that to increase its performances, a database stores data in memory buffers. But if the server crashes when the transaction is being committed, you’ll lose the data still in memory during the crash, which breaks the Durability of a transaction.</p></blockquote><p>我们已经看到，为了提升性能，数据库将数据存在内存中的缓冲区。
但是如果在事务提交后崩溃，你将会丢失内存中的所有数据，这会打破事务的持久性。</p><blockquote><p>You can write everything on disk but if the server crashes, you’ll end up with the data half written on disk, which breaks the Atomicity of a transaction.</p></blockquote><p>你可以把任何东西写入磁盘，但如果此时服务器崩溃，你的数据在磁盘上将处于半写入状态，会导致事务的原子性被破坏。</p><blockquote><p>Any modification written by a transaction must be undone or finished.</p></blockquote><p><strong>任何事务的修改都应该是0%或者100%。</strong></p><blockquote><p>To deal with this problem, there are 2 ways:</p><ul><li>Shadow copies/pages: Each transaction creates its own copy of the database (or just a part of the database) and works on this copy. In case of error, the copy is removed. In case of success, the database switches instantly the data from the copy with a filesystem trick then it removes the “old” data.</li><li>Transaction log: A transaction log is a storage space. Before each write on disk, the database writes an info on the transaction log so that in case of crash/cancel of a transaction, the database knows how to remove (or finish) the unfinished transaction.</li></ul></blockquote><p>为了解决这个问题有两种方式：</p><ul><li><strong>影子拷贝/页</strong>：每个事务会创建出数据库的一份自己的拷贝(只拷贝需要修改的部分)并且在这个拷贝上进行修改。在遇到错误的时候，移除拷贝即可。如果成功，数据库立刻将这部分数据的指针指向新的拷贝，将旧数据删除。</li><li><strong>事务日志</strong>：事务日志是一片存储空间。在每次写入磁盘之前，数据库将写操作的信息写入这份日志以应对事务的崩溃/取消，数据库知道如何移除(或完成)未完成的事务。</li></ul><p><strong>WAL</strong></p><blockquote><p>The shadow copies/pages creates a huge disk overhead when used on large databases involving many transactions. That’s why modern databases use a transaction log. The transaction log must be stored on a stable storage. I won’t go deeper on storage technologies but using (at least) RAID disks is mandatory to prevent from a disk failure.</p></blockquote><p>当在一个大型的数据库中运行很多事务的时候影子拷贝/页会造成巨额磁盘开销。
这也是为什么现代数据库使用<strong>事务日志</strong>。
事务日志必须存储在<strong>不易失</strong>存储上。
我不会深入介绍存储技术，但是必须使用RAID来避免磁盘故障。</p><blockquote><p>Most databases (at least Oracle, SQL Server, DB2, PostgreSQL, MySQL and SQLite) deal with the transaction log using the Write-Ahead Logging protocol (WAL). The WAL protocol is a set of 3 rules:</p><ul><li><ol><li>Each modification into the database produces a log record, and the log record must be written into the transaction log before the data is written on disk.</li></ol></li><li><ol start=2><li>The log records must be written in order; a log record A that happens before a log record B must but written before B</li></ol></li><li><ol start=3><li>When a transaction is committed, the commit order must be written on the transaction log before the transaction ends up successfully.</li></ol></li></ul></blockquote><p>大多数数据库使用 <strong>预写日志协议</strong> 来实现事务日志。
WAL协议由以下三条规则组成：</p><ul><li>每个对数据库的修改会产生一条日志记录，并且 <strong>日志记录需要比数据修改更先写入磁盘</strong>。</li><li>日志记录需要按序写入；发生在B之前的日志记录A必须要比B先写入</li><li>当事务提交的时候，提交这个操作作为一个日志需要先于事务成功结束写入日志记录</li></ul><figure><a class=lightgallery href=/images/log_manager.png title=/images/log_manager.png data-thumbnail=/images/log_manager.png data-sub-html="<h2>日志管理器</h2>"><img class=lazyload src=/svg/loading.min.svg data-src=/images/log_manager.png data-srcset="/images/log_manager.png, /images/log_manager.png 1.5x, /images/log_manager.png 2x" data-sizes=auto alt=/images/log_manager.png height=1500 width=1500></a><figcaption class=image-caption>日志管理器</figcaption></figure><blockquote><p>This job is done by a log manager. An easy way to see it is that between the cache manager and the data access manager (that writes data on disk) the log manager writes every update/delete/create/commit/rollback on the transaction log before they’re written on disk. Easy, right?</p></blockquote><p>这个工作是由事务管理器完成的。
一种简单的理解其定位的方式如图所示，位于缓存管理器和数据访问管理器之间，将所有的事务修改操作在写入磁盘前记录到事务日志中。
看起来很简单是吗？</p><blockquote><p>WRONG ANSWER! After all we’ve been through, you should know that everything related to a database is cursed by the “database effect”. More seriously, the problem is to find a way to write logs while keeping good performances. If the writes on the transaction log are too slow they will slow down everything.</p></blockquote><p>大错特错！
在看完这一部分之后你就会知道和数据库有关的一切东西都被“数据库性能”诅咒了。
更严谨的说法是，如何找到一种保证性能的前提下的写日志方式。
如果写事务日志很慢，它们会拖垮所有操作。</p><p><strong>ARIES</strong></p><blockquote><p>In 1992, IBM researchers “invented” an enhanced version of WAL called ARIES. ARIES is more or less used by most modern databases. The logic might not be the same but the concepts behind ARIES are used everywhere. I put the quotes on invented because, according to this MIT course, the IBM researchers did “nothing more than writing the good practices of transaction recovery”. Since I was 5 when the ARIES paper was published, I don’t care about this old gossip from bitter researchers. In fact, I only put this info to give you a break before we start this last technical part. I’ve read a huge part of the research paper on ARIES and I find it very interesting! In this part I’ll only give you an overview of ARIES but I strongly recommend to read the paper if you want a real knowledge.</p></blockquote><p>在1992年，IBM研究员“发明了”一种WAL的增强版本叫做ARIES。
在现代数据库中多多少少能找到ARIES的影子。
实现逻辑可能不同，但ARIES背后的理念到处在用。
之所以在发明一词加上引号，是因为MIT的课中说IBM的研究员“无非是找到一种事务恢复的良好实现”。
因为ARIES发布的时候我才5岁，所以我不care这些研究员的老八卦。
事实上，我提到这个信息只是为了在进入到本文最后一个部分前让你稍作休息。
我读了很大一部分<a href=https://www.cs.berkeley.edu/~brewer/cs262/Aries.pdf target=_blank rel="noopener noreffer">关于ARIES的研究</a>，我觉得蛮有趣的。
在这一章我仅介绍ARIES的概览，但如果你想学到真正的知识，我强烈建议你去读一下原文。</p><blockquote><p>ARIES stands for Algorithms for Recovery and Isolation Exploiting Semantics.</p><p>The aim of this technique is double:</p><ul><li><ol><li>Having good performances when writing logs</li></ol></li><li><ol start=2><li>Having a fast and reliable recovery</li></ol></li></ul></blockquote><p>ARIES是基于语义的恢复和隔离算法的缩写。</p><p>这个技术的主要目标有二：</p><ul><li><strong>在写日志的时候性能优秀</strong></li><li><strong>能进行快速、可靠的恢复</strong></li></ul><blockquote><p>There are multiple reasons a database has to rollback a transaction:</p><ul><li>Because the user cancelled it</li><li>Because of server or network failures</li><li>Because the transaction has broken the integrity of the database (for example you have a UNIQUE constraint on a column and the transaction adds a duplicate)</li><li>Because of deadlocks</li></ul></blockquote><p>数据库想要回滚一个事务有很多原因：</p><ul><li>因为用户取消</li><li>因为服务或者网络故障</li><li>因为事务打破了数据库的整体性约束</li><li>因为死锁</li></ul><blockquote><p>Sometimes (for example, in case of network failure), the database can recover the transaction.</p></blockquote><p>有时候(例如网络故障)，数据库能恢复事务。</p><blockquote><p>How is that possible? To answer this question, we need to understand the information stored in a log record.</p></blockquote><p>怎么做的？
为了回答这个问题，需要先理解存储在日志记录中的信息。</p><h1 id=6-总结>6 总结</h1></div><div class=post-footer id=post-footer><div class=post-info><div class=post-info-line><div class=post-info-mod><span>更新于 2022-05-22</span></div><div class=post-info-license></div></div><div class=post-info-line><div class=post-info-md></div><div class=post-info-share><span></span></div></div></div><div class=post-info-more><section class=post-tags><i class="fas fa-tags fa-fw"></i>&nbsp;<a href=/tags/db/>db</a>,&nbsp;<a href=/tags/rdbms/>rdbms</a></section><section><span><a href=javascript:void(0); onclick=window.history.back()>返回</a></span>&nbsp;|&nbsp;<span><a href=/>主页</a></span></section></div><div class=post-nav><a href=/posts/cmu15445-db-storage/ class=prev rel=prev title=[note]数据库存储><i class="fas fa-angle-left fa-fw"></i>[note]数据库存储</a></div></div><div id=comments><div id=gitalk class=comment></div><noscript>Please enable JavaScript to view the comments powered by <a href=https://github.com/gitalk/gitalk></a>Gitalk</a>.</noscript></div></article></div></main><footer class=footer><div class=footer-container><div class=footer-line>由 <a href=https://gohugo.io/ target=_blank rel="noopener noreffer" title="Hugo 0.99.1">Hugo</a> 强力驱动 | 主题 - <a href=https://github.com/dillonzq/LoveIt target=_blank rel="noopener noreffer" title="LoveIt 0.2.10"><i class="far fa-kiss-wink-heart fa-fw"></i> LoveIt</a></div><div class=footer-line><i class="far fa-copyright fa-fw"></i><span itemprop=copyrightYear>2022</span><span class=author itemprop=copyrightHolder>&nbsp;<a href=/ target=_blank>zh</a></span></div></div></footer></div><div id=fixed-buttons><a href=# id=back-to-top class=fixed-button title=回到顶部><i class="fas fa-arrow-up fa-fw"></i>
</a><a href=# id=view-comments class=fixed-button title=查看评论><i class="fas fa-comment fa-fw"></i></a></div><link rel=stylesheet href=/lib/gitalk/gitalk.min.css><link rel=stylesheet href=/lib/katex/katex.min.css><link rel=stylesheet href=/lib/katex/copy-tex.min.css><link rel=stylesheet href=/css/befdf6.min.css><script type=text/javascript src=/lib/gitalk/gitalk.min.js></script><script type=text/javascript src=/lib/smooth-scroll/smooth-scroll.min.js></script><script type=text/javascript src=/lib/lazysizes/lazysizes.min.js></script><script type=text/javascript src=/lib/clipboard/clipboard.min.js></script><script type=text/javascript src=/lib/sharer/sharer.min.js></script><script type=text/javascript src=/lib/katex/katex.min.js></script><script type=text/javascript src=/lib/katex/auto-render.min.js></script><script type=text/javascript src=/lib/katex/copy-tex.min.js></script><script type=text/javascript src=/lib/katex/mhchem.min.js></script><script type=text/javascript>window.config={code:{copyTitle:"复制到剪贴板",maxShownLines:10},comment:{gitalk:{admin:["rustzzh"],clientID:"da0c5fc5ea73e5fd68b1",clientSecret:"b109d474223d93fb730ca7e01f2b4d0d63fcc60d",id:"2022-05-09T17:35:56+08:00",owner:"rustzzh",repo:"rustzzh.github.io",title:"[tran]关系型数据库原理"}},math:{delimiters:[{display:!0,left:"$$",right:"$$"},{display:!0,left:"\\[",right:"\\]"},{display:!1,left:"$",right:"$"},{display:!1,left:"\\(",right:"\\)"}],strict:!1}}</script><script type=text/javascript src=/js/theme.min.js></script></body></html>